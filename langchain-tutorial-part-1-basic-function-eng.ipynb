{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6954310,"sourceType":"datasetVersion","datasetId":3994243}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### LangChain\nLangChain is an innovative framework for developing applications based on large-scale language models (LLMs), a tool that allows you to effectively perform various tasks with language models. With its modern and modular structure, LangChain is primarily available in two versions, Python and Typescript/Javascript, which allows it to be used in a variety of environments.\n\nIt consists of several modules that work together to effectively connect language models and data sources, and organize the workflow of an application.\nLangChain is composed of several modules, which work together to effectively connect language models and data sources, and perform a variety of tasks. Each module provides flexibility through modularized abstraction and implementation, and users can combine components to customize existing chains or create new ones.\n\n### Main Modules\n\n1. **Model I/O (Input/Output):** Manages interaction with the language model, manipulating prompts and extracting information from the language model's output.\n\n2. **Data Connections:** Provides the essential building blocks for loading, transforming, storing, and querying application-specific data.\n\n3. **Chains:** Build the invocation order of operations and efficiently manage the various components chained together.\n\n4. **Agents:** Flexibly manipulate chains by deciding which tools to use when they receive a parent directive.\n\n5. **Memory:** Provides functionality that allows interactive systems to directly access past messages.\n\n6. **Callbacks:** Record and stream intermediate steps in the chain, utilized for logging, monitoring, and other tasks.\n\n\n### LangChain Use Cases\n\nLancechains can be applied to a variety of use cases. These include Q&A for documents, structured data analysis, API integration, code understanding, agent simulation, chatbot development, code writing, data extraction, graph data analysis, multimodal output, self-checking, summarization, tagging, and more. Lancechain also supports a variety of integrations to leverage callbacks, chat models, document loaders, databases, search methods, text embedding models, agent toolkits, tools, vector repositories, and more.\n\n\n### If you found this article helpful, please hit the UP button.\n\n### References\n\nhttps://github.com/gkamradt/langchain-tutorials\n\nhttps://github.com/wikibook/openai-llm\n","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n!pip install openai\n!pip install tiktoken\n!pip install sentence-transformers\n!pip install pypdf\n!pip install faiss-cpu","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-13T07:40:48.187149Z","iopub.execute_input":"2023-11-13T07:40:48.187540Z","iopub.status.idle":"2023-11-13T07:42:49.769559Z","shell.execute_reply.started":"2023-11-13T07:40:48.187509Z","shell.execute_reply":"2023-11-13T07:42:49.767996Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/15/f9/e79403efb880425babaa91f8ab19e1b6218dc694551fa6e7f40197b5a72a/langchain-0.0.335-py3-none-any.whl.metadata\n  Downloading langchain-0.0.335-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.7.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.1)\nCollecting jsonpatch<2.0,>=1.33 (from langchain)\n  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n  Obtaining dependency information for langsmith<0.1.0,>=0.0.63 from https://files.pythonhosted.org/packages/8f/30/bd8b1c22488e7ed1ac0bf3ae84cedc76ab18e236270fed6926801b4af383/langsmith-0.0.63-py3-none-any.whl.metadata\n  Downloading langsmith-0.0.63-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.5.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.7.22)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (3.0.9)\nDownloading langchain-0.0.335-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langsmith-0.0.63-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jsonpatch, langsmith, langchain\n  Attempting uninstall: jsonpatch\n    Found existing installation: jsonpatch 1.32\n    Uninstalling jsonpatch-1.32:\n      Successfully uninstalled jsonpatch-1.32\nSuccessfully installed jsonpatch-1.33 langchain-0.0.335 langsmith-0.0.63\nCollecting openai\n  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/5d/24/5d545ec04afb9d2f7e528860e11b8c8db6d7a955077437a7c00f4bdc817c/openai-1.2.3-py3-none-any.whl.metadata\n  Downloading openai-1.2.3-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: anyio<4,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (3.7.1)\nCollecting distro<2,>=1.7.0 (from openai)\n  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\nCollecting httpx<1,>=0.23.0 (from openai)\n  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/82/61/a5fca4a1e88e40969bbd0cf0d981f3aa76d5057db160b94f49603fc18740/httpx-0.25.1-py3-none-any.whl.metadata\n  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.10.12)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.5 in /opt/conda/lib/python3.10/site-packages (from openai) (4.5.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\nCollecting httpcore (from httpx<1,>=0.23.0->openai)\n  Obtaining dependency information for httpcore from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\nDownloading openai-1.2.3-py3-none-any.whl (220 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.25.1-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: httpcore, distro, httpx, openai\nSuccessfully installed distro-1.8.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.3\nCollecting tiktoken\n  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/f4/2e/0adf6e264b996e263b1c57cad6560ffd5492a69beb9fd779ed0463d486bc/tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.8.8)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\nDownloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.5.1\nCollecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.35.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.24.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.17.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=adbcad5d9d6284beb227c46aa0e0777ed5c702e78ce79254af971d6574defa63\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\nRequirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (3.17.0)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.7.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport getpass\n\nopenai_api_key = os.getenv('OPENAI_API_KEY', getpass.getpass(\"api\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:42:49.773328Z","iopub.execute_input":"2023-11-13T07:42:49.773858Z","iopub.status.idle":"2023-11-13T07:42:54.781648Z","shell.execute_reply.started":"2023-11-13T07:42:49.773809Z","shell.execute_reply":"2023-11-13T07:42:54.780731Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdin","text":"api ···················································\n"}]},{"cell_type":"markdown","source":"### Chat Message\n\nThe most commonly used Chat Messege allows you to invoke OpenAI's gpt3.5 and gpt4. \nTo do so, you need to use the message feature, which has three roles: System: background context to tell the AI what to do, Human: user message, and AI: detailed message to show the AI's response.\n\nYou can also get results from multiple lists at once, or use Function calling in the same way. ","metadata":{}},{"cell_type":"code","source":"from langchain.globals import set_llm_cache\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.cache import InMemoryCache\nfrom langchain.schema import HumanMessage, SystemMessage\n\n# chat features # chat message\n# System: background context that tells the AI what to do\n# Human: User message\n# AI: Detailed message showing what the AI responded to.\nchat = ChatOpenAI(temperature=.7,\n                  callbacks=([StreamingStdOutCallbackHandler()]),  # 콜백 기능 지원\n                  streaming=True,\n                  verbose=True,\n                  openai_api_key=openai_api_key\n                  )\n\n# simple\nresponse = chat([\n    SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n    HumanMessage(content=\"I like tomatoes, what should I eat?\")\n])\nprint(response)\n\n\n# Making multiple calls at once\nmessage_list = [HumanMessage(content=\"고양이 이름 지어줘\"),\n                HumanMessage(content=\"개 이름 지어줘\")]\nbatch = chat.generate([message_list]) # generate로 한번에 생성한다.\nprint(batch)\n\n\n# Function calling\noutput = chat(messages=\n     [\n         SystemMessage(content=\"You are an helpful AI bot\"),\n         HumanMessage(content=\"What’s the weather like in Boston right now?\")\n     ],\n\n     functions=[{\n         \"name\": \"get_current_weather\",\n         \"description\": \"Get the current weather in a given location\",\n         \"parameters\": {\n             \"type\": \"object\",\n             \"properties\": {\n                 \"location\": {\n                     \"type\": \"string\",\n                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n                 },\n                 \"unit\": {\n                     \"type\": \"string\",\n                     \"enum\": [\"celsius\", \"fahrenheit\"]\n                 }\n             },\n             \"required\": [\"location\"]\n         }\n     }\n     ]\n)\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:42:54.783271Z","iopub.execute_input":"2023-11-13T07:42:54.783925Z","iopub.status.idle":"2023-11-13T07:43:01.849501Z","shell.execute_reply.started":"2023-11-13T07:42:54.783879Z","shell.execute_reply":"2023-11-13T07:43:01.847789Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"You could try a Caprese salad with fresh tomatoes, mozzarella, and basil.content='You could try a Caprese salad with fresh tomatoes, mozzarella, and basil.'\n1. 민트\n2. 코코\n3. 루나\n4. 마루\n5. 토리\n6. 미미\n7. 레오\n8. 타이거\n9. 오렌지\n10. 삼돌이generations=[[ChatGenerationChunk(text='1. 민트\\n2. 코코\\n3. 루나\\n4. 마루\\n5. 토리\\n6. 미미\\n7. 레오\\n8. 타이거\\n9. 오렌지\\n10. 삼돌이', generation_info={'finish_reason': 'stop'}, message=AIMessageChunk(content='1. 민트\\n2. 코코\\n3. 루나\\n4. 마루\\n5. 토리\\n6. 미미\\n7. 레오\\n8. 타이거\\n9. 오렌지\\n10. 삼돌이'))]] llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('2743148c-b4a6-4f64-8a6a-8e70e97b5b24'))]\ncontent='' additional_kwargs={'function_call': {'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}', 'name': 'get_current_weather'}}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### In memory\n\nThe in-memory feature has the ability to store and use a global variable memory cache. You can use it in your LLM model to declare the cache as True False and use it as needed. It gives you the exact same answer to a question you've already created once. ","metadata":{}},{"cell_type":"code","source":"import time \n\n# 전역변수에 메모리 캐시\nchat = ChatOpenAI(temperature=.7,\n                  callbacks=([StreamingStdOutCallbackHandler()]),  # 콜백 기능 지원\n                  streaming=True,\n                  verbose=True,\n                  openai_api_key=openai_api_key\n                  )\n\nset_llm_cache(InMemoryCache())\nstart = time.time()\nprint(chat.generate([[HumanMessage(content=\"고양이 이름 지어줘\")]]))\nend = time.time()\nprint(end-start) # 2초\n\nstart = time.time()\nprint(chat.generate([[HumanMessage(content=\"고양이 이름 지어줘\")]]))\nend = time.time()\nprint(end-start) # 0.0019991397857666016초","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:01.854385Z","iopub.execute_input":"2023-11-13T07:43:01.854972Z","iopub.status.idle":"2023-11-13T07:43:04.427574Z","shell.execute_reply.started":"2023-11-13T07:43:01.854906Z","shell.execute_reply":"2023-11-13T07:43:04.426398Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1. 미야\n2. 코코\n3. 오레오\n4. 토리\n5. 루나\n6. 찰리\n7. 피치\n8. 레오\n9. 민트\n10. 쿠키generations=[[ChatGenerationChunk(text='1. 미야\\n2. 코코\\n3. 오레오\\n4. 토리\\n5. 루나\\n6. 찰리\\n7. 피치\\n8. 레오\\n9. 민트\\n10. 쿠키', generation_info={'finish_reason': 'stop'}, message=AIMessageChunk(content='1. 미야\\n2. 코코\\n3. 오레오\\n4. 토리\\n5. 루나\\n6. 찰리\\n7. 피치\\n8. 레오\\n9. 민트\\n10. 쿠키'))]] llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('78103fd1-7f3d-4917-8467-19e6f898858f'))]\n2.3479416370391846\ngenerations=[[ChatGenerationChunk(text='1. 미야\\n2. 코코\\n3. 오레오\\n4. 토리\\n5. 루나\\n6. 찰리\\n7. 피치\\n8. 레오\\n9. 민트\\n10. 쿠키', generation_info={'finish_reason': 'stop'}, message=AIMessageChunk(content='1. 미야\\n2. 코코\\n3. 오레오\\n4. 토리\\n5. 루나\\n6. 찰리\\n7. 피치\\n8. 레오\\n9. 민트\\n10. 쿠키'))]] llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('c5d2ce57-6a17-4130-8f08-32a66663e106'))]\n0.0010640621185302734\n","output_type":"stream"}]},{"cell_type":"code","source":"chat = ChatOpenAI(temperature=.7,\n                  callbacks=([StreamingStdOutCallbackHandler()]),  # 콜백 기능 지원\n                  streaming=True,\n                  verbose=True,\n                  openai_api_key=openai_api_key,\n                  cache=False # False로 사용\n                  )\n\nset_llm_cache(InMemoryCache())\nstart = time.time()\nprint(chat.generate([[HumanMessage(content=\"고양이 이름 지어줘\")]]))\nend = time.time()\nprint(end - start)  # 2초\n\nstart = time.time()\nprint(chat.generate([[HumanMessage(content=\"고양이 이름 지어줘\")]]))\nend = time.time()\nprint(end - start)  # 2초","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:04.428840Z","iopub.execute_input":"2023-11-13T07:43:04.429171Z","iopub.status.idle":"2023-11-13T07:43:10.283800Z","shell.execute_reply.started":"2023-11-13T07:43:04.429142Z","shell.execute_reply":"2023-11-13T07:43:10.282624Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"1. 미야 (Miya)\n2. 레오 (Leo)\n3. 루나 (Luna)\n4. 코코 (Coco)\n5. 토마스 (Thomas)\n6. 블루 (Blue)\n7. 다이아 (Dia)\n8. 나비 (Navi)\n9. 토리 (Tori)\n10. 씨앗 (C.C)generations=[[ChatGenerationChunk(text='1. 미야 (Miya)\\n2. 레오 (Leo)\\n3. 루나 (Luna)\\n4. 코코 (Coco)\\n5. 토마스 (Thomas)\\n6. 블루 (Blue)\\n7. 다이아 (Dia)\\n8. 나비 (Navi)\\n9. 토리 (Tori)\\n10. 씨앗 (C.C)', generation_info={'finish_reason': 'stop'}, message=AIMessageChunk(content='1. 미야 (Miya)\\n2. 레오 (Leo)\\n3. 루나 (Luna)\\n4. 코코 (Coco)\\n5. 토마스 (Thomas)\\n6. 블루 (Blue)\\n7. 다이아 (Dia)\\n8. 나비 (Navi)\\n9. 토리 (Tori)\\n10. 씨앗 (C.C)'))]] llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('2826d826-86b9-4768-8dba-df1942dcb5cf'))]\n2.8188323974609375\n1. 미야 (Miya)\n2. 코코 (Coco)\n3. 레오 (Leo)\n4. 루나 (Luna)\n5. 민트 (Mint)\n6. 오렌지 (Orange)\n7. 라일리 (Riley)\n8. 소피 (Sophie)\n9. 토리 (Tori)\n10. 맥스 (Max)generations=[[ChatGenerationChunk(text='1. 미야 (Miya)\\n2. 코코 (Coco)\\n3. 레오 (Leo)\\n4. 루나 (Luna)\\n5. 민트 (Mint)\\n6. 오렌지 (Orange)\\n7. 라일리 (Riley)\\n8. 소피 (Sophie)\\n9. 토리 (Tori)\\n10. 맥스 (Max)', generation_info={'finish_reason': 'stop'}, message=AIMessageChunk(content='1. 미야 (Miya)\\n2. 코코 (Coco)\\n3. 레오 (Leo)\\n4. 루나 (Luna)\\n5. 민트 (Mint)\\n6. 오렌지 (Orange)\\n7. 라일리 (Riley)\\n8. 소피 (Sophie)\\n9. 토리 (Tori)\\n10. 맥스 (Max)'))]] llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('0dc0bbde-db8c-4389-bf21-3549197eeadb'))]\n2.9796030521392822\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### embedding\n\nThere is a function that uses embeddings. Rather than using embeddings directly, they are often used as arguments passed to create Vector indices, and OpenAI's embeddings are often used, but it is also recommended to use the embeddings provided by the huggingface to save money. ","metadata":{}},{"cell_type":"code","source":"\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# 임베딩 모델\nembeddings = OpenAIEmbeddings(model='text-embedding-ada-002', openai_api_key=openai_api_key)\n\ntext = \"안녕하세요! 해변에 갈 시간입니다\"\ntext_embedding = embeddings.embed_query(text)\n\nprint (f\"임베딩 길이 : {len(text_embedding)}\")\nprint (f\"샘플은 다움과 같습니다 : {text_embedding[:5]}...\")\n\n# 다중 문서 임베딩\nembeddings = OpenAIEmbeddings(model='text-embedding-ada-002', openai_api_key=openai_api_key)\ntext_embedding = embeddings.embed_documents(\n    [\n        \"Hi there!\",\n        \"Oh, hello!\",\n        \"What's your name?\",\n        \"My friends call me World\",\n        \"Hello World!\"\n    ]\n)\nprint(f\"임베딩 길이 : {len(text_embedding)}, {len(text_embedding[0])}\")\nprint(f\"샘플은 다움과 같습니다 : {text_embedding[0][:5]}...\")\n\n\n# 허깅페이스 모델 센텐스 트랜스포머 임베딩 # pip install sentence_transformers\nhf_embeddings = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n    model_kwargs={'device': 'cpu'}, # 모델의 전달할 키워드 인수\n    # encode_kwargs={'normalize_embeddings': False},  # 모델의 `encode` 메서드를 호출할 때 전달할 키워드 인수\n)\ntext = \"안녕하세요! 해변에 갈 시간입니다\"\ntext_embedding = hf_embeddings.embed_query(text)\nprint (f\"임베딩 길이 : {len(text_embedding)}\")\nprint (f\"샘플은 다움과 같습니다 : {text_embedding[:5]}...\")","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:10.285890Z","iopub.execute_input":"2023-11-13T07:43:10.286708Z","iopub.status.idle":"2023-11-13T07:43:29.801542Z","shell.execute_reply.started":"2023-11-13T07:43:10.286658Z","shell.execute_reply":"2023-11-13T07:43:29.800396Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"임베딩 길이 : 1536\n샘플은 다움과 같습니다 : [0.0062775725893334435, -0.02553328215339524, -0.01215778989813698, -0.013727984797761118, -0.02054711352729209]...\n임베딩 길이 : 5, 1536\n샘플은 다움과 같습니다 : [-0.020262643931117454, -0.006984279861728337, -0.022630838723440946, -0.02634143617913019, -0.03697932214749123]...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)99753/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6d8db2e56a4c009dd3fe964bdbbc29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9422e46537524ae5baef9778a4dd61d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)0cdb299753/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4d033e99b44c51965f3f4bd0f722d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)db299753/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba293170d094bdab175e17a517dcfea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2e4a0aca2d54bbfb1f693b81a5569c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)753/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ca4748ce43409a854cafb4f5c6f0f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01db2e7408754c49ad5a03c2627dc664"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee08b922deb94c7da70a84808f81f61d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"724aa13051904a65b7f51c3c0b176757"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)99753/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0eb7bafe1bc466dac6ade0c8852e005"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf42717134ed4296bcdcae6a44c77ff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)9753/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2593222429cd4fe4a8103c47f1715b17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)0cdb299753/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e6e70921c54481daff7deb66b2c6b8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)b299753/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58aff22898394132a1de4fa99c2e0cb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24978ac5e2d545b691d9991e21f0b1ad"}},"metadata":{}},{"name":"stdout","text":"임베딩 길이 : 768\n샘플은 다움과 같습니다 : [0.011741535738110542, -0.0317050963640213, -0.013708160258829594, 0.013744894415140152, 0.04403567686676979]...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### prompt template\n\nDefine and use a prompt template. You can use them for anything from prompts with no input variables to prompts with multiple input variables. \nOnce you've defined a template and given it arguments that become variables via {}, you can continue to use it via format.","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate, PromptTemplate\n\n# 입력 변수가 없는 프롬프트 예제\nno_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\nprompt = no_input_prompt.format()\nprint(prompt)\n\n\n# 하나의 입력 변수가 있는 예제 프롬프트\none_input_prompt = PromptTemplate(template=\"Tell me a {adjective} joke.\", input_variables=[\"adjective\"],)\nprompt = one_input_prompt.format(adjective=\"funny\")\nprint(prompt)\n\n\n# 여러 입력 변수가 있는 프롬프트 예제\nmultiple_input_prompt = PromptTemplate(template=\"Tell me a {adjective} joke about {content}.\",\n                                       input_variables=[\"adjective\", \"content\"],\n                                       )\nprompt = multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")\nprint(prompt)\n\n\n# input_variables를 지정 안한 프롬프트 예제\nno_variable_prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\nprompt = no_variable_prompt.format(product=\"colorful socks\")\nprint(prompt)\n\n\n# ChatPrompt 예제\nchat_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{human_text}\"),\n])\n\nmessage = chat_prompt.format_messages(input_language=\"English\",\n                                      output_language=\"French\",\n                                      human_text=\"I love programming.\")\nprint(message)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:29.803041Z","iopub.execute_input":"2023-11-13T07:43:29.803595Z","iopub.status.idle":"2023-11-13T07:43:29.817466Z","shell.execute_reply.started":"2023-11-13T07:43:29.803563Z","shell.execute_reply":"2023-11-13T07:43:29.816158Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Tell me a joke.\nTell me a funny joke.\nTell me a funny joke about chickens.\nWhat is a good name for a company that makes colorful socks?\n[SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Output Parser\n\nTypically, LLM outputs text. However, you may want to get more structured information.\nIn this case, you can use an output parser to structure the LLM response.\nThe output parser has two concepts\n\n- Format instructions: You tell the LLM how you want the results to be formatted.\n- Parser: Tells it to extract the desired textual output structure (usually json).\n\nThis output parser allows users to specify an arbitrary JSON schema and query the LLM for JSON output that conforms to that schema.\n\nKeep in mind that large language models are leaky abstractions! \nYou should use an LLM with sufficient capacity to generate well-formed JSON. \nIn the OpenAI suite, Da Vinci can be handled reliably, but Curry already has a steep performance drop-off.\nUse Pydantic to declare your data model. Pydantic's BaseModel is like a Python data class, but with real type checking + coercion.","metadata":{}},{"cell_type":"code","source":"from langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.schema import BaseOutputParser\nfrom langchain.output_parsers import (PydanticOutputParser,\n                                      OutputFixingParser,\n                                      RetryWithErrorOutputParser,\n                                      CommaSeparatedListOutputParser,\n                                      )\nfrom pydantic import BaseModel, Field, validator","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:29.819061Z","iopub.execute_input":"2023-11-13T07:43:29.819505Z","iopub.status.idle":"2023-11-13T07:43:29.902085Z","shell.execute_reply.started":"2023-11-13T07:43:29.819463Z","shell.execute_reply":"2023-11-13T07:43:29.900850Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCommaSeparatedListOutputParser\n\nYou tell the LLM how you want the results to be formatted by explicitly calling get_format_instructions.\nThe syntax below is passed as the query statement, and that's how many tokens are consumed.\n\"Your response should be a list of comma separated values, \"\n\"eg: `foo, bar, baz`\"\n\"\"\"\n\noutput_parser = CommaSeparatedListOutputParser()\nformat_instructions = output_parser.get_format_instructions()\n\nprompt = PromptTemplate(\n    template=\"List five {subject}.\\n{format_instructions}\",\n    input_variables=[\"subject\"],\n    partial_variables={\"format_instructions\": format_instructions}\n)\n\nmodel = OpenAI(temperature=0, openai_api_key=openai_api_key)\n_input = prompt.format(subject=\"ice cream flavors\") # 'List five ice cream flavors.\\nYour response should be a list of comma separated values, eg: `foo, bar, baz`'\noutput = model(_input)\nprint(output_parser.parse(output))","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:29.903592Z","iopub.execute_input":"2023-11-13T07:43:29.904309Z","iopub.status.idle":"2023-11-13T07:43:30.647279Z","shell.execute_reply.started":"2023-11-13T07:43:29.904273Z","shell.execute_reply":"2023-11-13T07:43:30.645988Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookies and Cream']\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nIt can be used similarly to a JSON parser / Function calling.\n\nPydantic makes it easy to add custom validation logic.\nPydantic's BaseModel is similar to a Python data class, but with real type checking + coercion.\n\"\"\"\n    \nclass Joke(BaseModel):\n\n    setup: str = Field(description=\"question to set up a joke\")\n    punchline: str = Field(description=\"answer to resolve the joke\")\n\n    @validator(\"setup\")\n    def question_ends_with_question_mark(cls, field):\n        if field[-1] != \"?\":\n            raise ValueError(\"Badly formed question!\")\n        return field\n\n\n\"\"\"\nYou tell the LLM how you want the results to be formatted by explicitly calling get_format_instructions.\nThe syntax below is passed as the query statement, and that's how many tokens are consumed.\n\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere is the output schema:\n```\n{schema}\n```\n\"\"\"\nparser = PydanticOutputParser(pydantic_object=Joke)\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nmodel = OpenAI(temperature=0,\n               callbacks=([StreamingStdOutCallbackHandler()]),\n               streaming=True ,\n               verbose=True,\n               openai_api_key=openai_api_key)\n\nprompt_and_model = prompt | model\noutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:30.659392Z","iopub.execute_input":"2023-11-13T07:43:30.659811Z","iopub.status.idle":"2023-11-13T07:43:31.339676Z","shell.execute_reply.started":"2023-11-13T07:43:30.659778Z","shell.execute_reply":"2023-11-13T07:43:31.338529Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\n{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}\n{\"setup\": \"Why did the chicken cross the road?\", \"punchline\": \"To get to the other side!\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"class Action(BaseModel):\n    action: str = Field(description=\"action to take\")\n    action_input: str = Field(description=\"input to the action\")\n\n\nparser = PydanticOutputParser(pydantic_object=Action)\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nprompt_value = prompt.format_prompt(query=\"who is leo di caprios gf?\")\nbad_response = '{\"action\": \"search\"}'\n# parser.parse(bad_response)\n\n\"\"\"\nIf you run parser.parse(bad_response), you will get an error because there is no error action_input. \n\nlangchain.schema.output_parser.OutputParserException: Failed to parse Action from completion {\"action\": \"search\"}. Got: 1 validation error for Action\naction_input\nfield required (type=value_error.missing)\n\"\"\"\n\nmodel = OpenAI(temperature=0, openai_api_key=openai_api_key)\n\n# Auto-Fixing Parser 활용\nfix_parser = OutputFixingParser.from_llm(parser=parser, llm=model)\noutput = fix_parser.parse(bad_response)\nprint(output)\n\n#대신, 프롬프트 (원래 출력뿐만 아니라)를 통과하는 RetryOutputParser를 사용하여 더 나은 응답을 얻기 위해 다시 시도 할 수 있습니다.\nretry_parser = RetryWithErrorOutputParser.from_llm(parser=parser, llm=model)\noutput = retry_parser.parse_with_prompt(bad_response, prompt_value)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:31.341271Z","iopub.execute_input":"2023-11-13T07:43:31.342115Z","iopub.status.idle":"2023-11-13T07:43:33.214546Z","shell.execute_reply.started":"2023-11-13T07:43:31.342076Z","shell.execute_reply":"2023-11-13T07:43:33.213429Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"action='search' action_input=''\naction='search' action_input='who is leo di caprios gf?'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Custom output parser\nIf you want to define a custom parser, you can do so by inheriting from BaseOutputParser. \nYou can create your own custom output parser by simply overriding the parser function. \nIn addition, if you define get_format_instructions and return it as a string, you can pass it to the prompt.","metadata":{}},{"cell_type":"code","source":"class CustomSpaceSeparatedListOutputParser(BaseOutputParser):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n    def parse(self, text: str):\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\" \")\n\n\n\nparser = CustomSpaceSeparatedListOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n\\n{query}\\n\",\n    input_variables=[\"query\"],\n)\n\nmodel = OpenAI(temperature=0,\n               callbacks=([StreamingStdOutCallbackHandler()]),\n               streaming=True ,\n               verbose=True,\n               openai_api_key=openai_api_key)\n\nprompt_and_model = prompt | model | parser\n\noutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:33.216412Z","iopub.execute_input":"2023-11-13T07:43:33.216845Z","iopub.status.idle":"2023-11-13T07:43:34.026153Z","shell.execute_reply.started":"2023-11-13T07:43:33.216805Z","shell.execute_reply":"2023-11-13T07:43:34.025307Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\nQ: What did the fish say when it hit the wall?\nA: Dam!['Q:', 'What', 'did', 'the', 'fish', 'say', 'when', 'it', 'hit', 'the', 'wall?\\nA:', 'Dam!']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Link Chain\n\nThe most common and valuable configurations are\n\nPromptTemplate / ChatPromptTemplate -> LLM / ChatModel -> OutputParser.\n\nAlmost every chain you build uses this building block.\n\nIt allows you to link chains with chains, and is very versatile. ","metadata":{}},{"cell_type":"code","source":"from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n\n# 프롬프트 템플릿 생성\nprompt = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"\"\"Q: {question}\\nA:\"\"\"\n)\n\n# LLMChain 생성\nllm_chain = LLMChain(\n    llm=model,\n    prompt=prompt,\n    verbose=True\n)\n\n# LLMChain 실행\nquestion = \"기타를 잘 치는 방법은?\"\nprint(llm_chain.predict(question=question))","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:34.027221Z","iopub.execute_input":"2023-11-13T07:43:34.027539Z","iopub.status.idle":"2023-11-13T07:43:39.153198Z","shell.execute_reply.started":"2023-11-13T07:43:34.027510Z","shell.execute_reply":"2023-11-13T07:43:39.152010Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mQ: 기타를 잘 치는 방법은?\nA:\u001b[0m\n 기타를 잘 치는 방법은 다음과 같습니다.\n\n1. 손가락 위치를 잘 잡아야 합니다. 기타는 손가락의 위치에 따라 소리가 달라집니다.\n\n2. 손가락을 잘 움직여야 합니다. 손가락을 잘 움직이면 소리가 더 잘 납니다.\n\n3. 손가락의 압력\n\u001b[1m> Finished chain.\u001b[0m\n 기타를 잘 치는 방법은 다음과 같습니다.\n\n1. 손가락 위치를 잘 잡아야 합니다. 기타는 손가락의 위치에 따라 소리가 달라집니다.\n\n2. 손가락을 잘 움직여야 합니다. 손가락을 잘 움직이면 소리가 더 잘 납니다.\n\n3. 손가락의 압력\n","output_type":"stream"}]},{"cell_type":"code","source":"# LangChain Expression Language (LCEL) \n# The LangChain Expression Language is a declarative way to organize chains and provide built-in streaming, batch, and asynchronous support. \n# LCEL makes it easier to use LangChain.\n# It's a simple way to connect via |, the difference being that it uses invoke.\n\nprompt = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"\"\"Q: {question}\\nA:\"\"\"\n)\nquestion = \"기타를 잘 치는 방법은?\"\nparser = CommaSeparatedListOutputParser()\nchain = prompt | model | parser\noutput = chain.invoke({\"question\": question})\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:39.154896Z","iopub.execute_input":"2023-11-13T07:43:39.155254Z","iopub.status.idle":"2023-11-13T07:43:39.165232Z","shell.execute_reply.started":"2023-11-13T07:43:39.155223Z","shell.execute_reply":"2023-11-13T07:43:39.163871Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"['기타를 잘 치는 방법은 다음과 같습니다.\\n\\n1. 손가락 위치를 잘 잡아야 합니다. 기타는 손가락의 위치에 따라 소리가 달라집니다.\\n\\n2. 손가락을 잘 움직여야 합니다. 손가락을 잘 움직이면 소리가 더 잘 납니다.\\n\\n3. 손가락의 압력']\n","output_type":"stream"}]},{"cell_type":"code","source":"# SimpleSequentialChain\n\ntemplate1 = \"\"\"당신은 극작가입니다. 연극 제목이 주어졌을 때, 그 줄거리를 작성하는 것이 당신의 임무입니다.\n\n제목:{title}\n시놉시스:\"\"\"\nprompt1 = PromptTemplate(input_variables=[\"title\"], template=template1)\nchain1 = LLMChain(llm=model, prompt=prompt1)\n\ntemplate2 = \"\"\"당신은 연극 평론가입니다. 연극의 시놉시스가 주어지면 그 리뷰를 작성하는 것이 당신의 임무입니다.\n\n시놉시스:\n{synopsis}\n리뷰:\"\"\"\nprompt2 = PromptTemplate(input_variables=[\"synopsis\"], template=template2)\nchain2 = LLMChain(llm=model,prompt=prompt2)\n\n# SimpleSequentialChain으로 두 개의 체인을 연결\noverall_chain = SimpleSequentialChain(\n    chains=[chain1, chain2],\n    verbose=True\n)\nprint(overall_chain(\"서울 랩소디\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:39.166936Z","iopub.execute_input":"2023-11-13T07:43:39.167416Z","iopub.status.idle":"2023-11-13T07:43:47.911829Z","shell.execute_reply.started":"2023-11-13T07:43:39.167371Z","shell.execute_reply":"2023-11-13T07:43:47.910457Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n\n서울 랩소디는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 서울의 밤을 여행하며 새로운 인연과 만남을 경험합니다. 그녀는 새로운 사람들과 함께 서울의 밤을 즐기며, 새로운\u001b[36;1m\u001b[1;3m\n서울 랩소디는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 서울의 밤을 여행하며 새로운 인연과 만남을 경험합니다. 그녀는 새로운 사람들과 함께 서울의 밤을 즐기며, 새로운\u001b[0m\n\n\n\"서울 랩소디\"는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 서울의 밤을 여행하며 새로운 인연과 만남을 경험합니다. 이 연극은 서울의 밤을 배경으로 하는 이야기를 다루고 있\u001b[33;1m\u001b[1;3m\n\n\"서울 랩소디\"는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 서울의 밤을 여행하며 새로운 인연과 만남을 경험합니다. 이 연극은 서울의 밤을 배경으로 하는 이야기를 다루고 있\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n{'input': '서울 랩소디', 'output': '\\n\\n\"서울 랩소디\"는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 서울의 밤을 여행하며 새로운 인연과 만남을 경험합니다. 이 연극은 서울의 밤을 배경으로 하는 이야기를 다루고 있'}\n","output_type":"stream"}]},{"cell_type":"code","source":"template1 = \"\"\"당신은 극작가입니다. 연극 제목이 주어졌을 때, 그 줄거리를 작성하는 것이 당신의 임무입니다.\n\n제목:{title}\n시놉시스:\"\"\"\nprompt1 = PromptTemplate(input_variables=[\"title\"], template=template1)\nchain1 = LLMChain(llm=model, prompt=prompt1, output_key=\"synopsis\")\n\ntemplate2 = \"\"\"당신은 연극 평론가입니다. 연극의 시놉시스가 주어지면 그 리뷰를 작성하는 것이 당신의 임무입니다.\n\n시놉시스:\n{synopsis}\n리뷰:\"\"\"\nprompt2 = PromptTemplate(input_variables=[\"synopsis\"], template=template2)\nchain2 = LLMChain(llm=model, prompt=prompt2, output_key=\"review\")\n\noverall_chain = SequentialChain(\n    chains=[chain1, chain2],\n    input_variables=[\"title\"],\n    output_variables=[\"synopsis\", \"review\"],\n    verbose=True\n)\nprint(overall_chain(\"서울 랩소디\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:47.913247Z","iopub.execute_input":"2023-11-13T07:43:47.913576Z","iopub.status.idle":"2023-11-13T07:43:47.926571Z","shell.execute_reply.started":"2023-11-13T07:43:47.913547Z","shell.execute_reply":"2023-11-13T07:43:47.925274Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n{'title': '서울 랩소디', 'synopsis': '\\n서울 랩소디는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 서울의 밤을 여행하며 새로운 인연과 만남을 경험합니다. 그녀는 새로운 사람들과 함께 서울의 밤을 즐기며, 새로운', 'review': '\\n\\n\"서울 랩소디\"는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 서울의 밤을 여행하며 새로운 인연과 만남을 경험합니다. 이 연극은 서울의 밤을 배경으로 하는 이야기를 다루고 있'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Text Split\n\nThis function allows you to split text into chunks. You can cut it into characters, and you can cut it to a size that fits your tokenizer. You can also split documents like HTML into tags. ","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import CharacterTextSplitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.text_splitter import HTMLHeaderTextSplitter\nfrom transformers import GPT2TokenizerFast\n\nwith open('/kaggle/input/langchain-tutorial/akazukin_all.txt', encoding='utf-8') as f:\n    akazukin_all = f.read()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:47.928460Z","iopub.execute_input":"2023-11-13T07:43:47.929431Z","iopub.status.idle":"2023-11-13T07:43:47.960777Z","shell.execute_reply.started":"2023-11-13T07:43:47.929382Z","shell.execute_reply":"2023-11-13T07:43:47.959504Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\n# separator로 자르고 글자 수에 맞춰서 자른다.\ntext_splitter = CharacterTextSplitter(\n    separator = \"\\n\\n\",\n    chunk_size = 20,\n    chunk_overlap  = 2,\n    length_function = len,\n    is_separator_regex = False,\n)\n\ntexts = text_splitter.split_text(akazukin_all)\nprint(texts)\n\n\n# \"\\n\\n\", \"\\n\", \" \", \"\"을 알아서 찾아서 리컬시브하게 알아서, 나눠서 글수 단위로 자른다.\ntext_splitter = RecursiveCharacterTextSplitter(\n    # separator=\"\\n\\n\" 따로 지정해줄 필요가 없다.\n    chunk_size = 20,\n    chunk_overlap  = 2,\n    length_function = len,\n    is_separator_regex = False,\n)\ntexts = text_splitter.split_text(akazukin_all)\nprint(texts)\n\n\n# 토큰 베이스 스플릿\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ntext_splitter = CharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=20, chunk_overlap=2)\ntexts = text_splitter.split_text(akazukin_all)\nprint(texts)\n\n\n# HTML 스플릿\nhtml_string = \"\"\"\n<!DOCTYPE html>\n<html>\n<body>\n    <div>\n        <h1>Foo</h1>\n        <p>Some intro text about Foo.</p>\n        <div>\n            <h2>Bar main section</h2>\n            <p>Some intro text about Bar.</p>\n            <h3>Bar subsection 1</h3>\n            <p>Some text about the first subtopic of Bar.</p>\n            <h3>Bar subsection 2</h3>\n            <p>Some text about the second subtopic of Bar.</p>\n        </div>\n        <div>\n            <h2>Baz</h2>\n            <p>Some text about Baz</p>\n        </div>\n        <br>\n        <p>Some concluding text about Foo</p>\n    </div>\n</body>\n</html>\n\"\"\"\n\n# 헤드만 따고 싶다.\nheaders_to_split_on = [\n    (\"h1\", \"Header 1\"),\n    (\"h2\", \"Header 2\"),\n    (\"h3\", \"Header 3\"),\n]\n\nhtml_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on) # pip install lxml\nhtml_header_splits = html_splitter.split_text(html_string)\nprint(html_header_splits)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:47.962665Z","iopub.execute_input":"2023-11-13T07:43:47.963074Z","iopub.status.idle":"2023-11-13T07:43:50.104652Z","shell.execute_reply.started":"2023-11-13T07:43:47.963041Z","shell.execute_reply":"2023-11-13T07:43:50.103538Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[\"제목: '전뇌 빨간 망토'\", '제1장: 데이터 프론트', '밤이 되면 반짝이는 네오 도쿄. 고층 빌딩이 늘어서고, 네온사인이 거리를 수놓는다. 그 거리에서 빨간 두건을 쓴 소녀 미코는 불법 데이터 카우리아를 운반하는 배달원으로 일하고 있었다. 그녀는 어머니가 병에 걸려 치료비를 벌기 위해 데이터카우리아에 몸을 던지고 있었다.', \"그러던 어느 날, 미코는 중요한 데이터를 운반하는 임무를 맡게 된다. 그 데이터에는 거대 기업 '울프 코퍼레이션'의 시민에 대한 악랄한 지배를 폭로하는 정보가 담겨 있었다. 그녀는 데이터를 받아 목적지로 향한다.\", '제2장: 울프 코퍼레이션의 함정', \"미코는 목적지인 술집 '할머니의 집'으로 향하는 길에 울프 코퍼레이션의 요원들에게 쫓기게 된다. 그들은 '빨간 망토'라는 데이터 카우리아에 대한 소문을 듣고 데이터를 탈취하려 했다. 미코는 교묘하게 요원들을 흩뿌리고 술집에 도착한다.\", '제3장: 배신과 재회', \"술집 '할머니의 집'에서 미코는 데이터를 받을 사람인 료를 기다리고 있었다. 료는 그녀의 어릴 적 친구이자 그 역시 울프 코퍼레이션과 싸우는 해커 집단의 일원이었다. 하지만 료는 미코에게 배신감을 느꼈고, 그녀가 데이터 카우리아에 몸을 던진 것에 화가 났다.\", '그럼에도 불구하고 미코는 료에게 데이터를 건네며 울프 코퍼레이션에 대한 반격을 믿기로 한다. 두 사람은 함께 울프 코퍼레이션의 음모를 밝혀내고 시민들을 구하기로 결심한다.', '제4장: 울프 코퍼레이션의 붕괴', '미코와 료는 해커 집단과 함께 울프 코퍼레이션에 대한 최후의 결전을 벌인다. 능숙한 해킹 기술과 신체 능력으로 그들은 울프 코퍼레이션의 보안을 차례로 뚫어 나간다. 그 과정에서 미코는 울프 코퍼레이션이 어머니의 병에 관여하고 있다는 사실을 알게 된다. 그녀는 분노에 휩싸여 울프코퍼레이션에 대한 복수를 다짐한다.', '제5장: 결전의 순간', '미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신의 압도적인 힘을 과시한다. 하지만 미코와 료는 서로를 도와가며 울프 박사와 싸우고 그의 약점을 찾아낸다.', '제6장: 진실의 해방', '미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다. 그리고 해커 집단과 함께 울프 코퍼레이션의 악행을 세상에 공개하고 시민들을 해방시킨다. 이 승리로 미코의 어머니의 치료법도 찾아내고, 그녀의 병은 완치된다.', '제7장: 새로운 시작', '울프 코퍼레이션이 무너진 후, 미코와 료는 서로의 과거를 용서하고 다시 우정을 회복한다. 미코는 데이터카우리아를 그만두고 료와 함께 새로운 길을 걷기 시작한다. 그들은 스스로의 힘으로 미래의 네오 도쿄를 더 나은 도시로 바꾸어 나갈 것을 다짐한다. 이것은 미코와 료, 그리고 전뇌 빨간 망토의 새로운 모험의 시작이었다.', '결말']\n[\"제목: '전뇌 빨간 망토'\", '제1장: 데이터 프론트', '밤이 되면 반짝이는 네오 도쿄.', '고층 빌딩이 늘어서고, 네온사인이', '거리를 수놓는다. 그 거리에서 빨간', '두건을 쓴 소녀 미코는 불법 데이터', '카우리아를 운반하는 배달원으로', '일하고 있었다. 그녀는 어머니가', '병에 걸려 치료비를 벌기 위해', '데이터카우리아에 몸을 던지고', '있었다.', '그러던 어느 날, 미코는 중요한', '데이터를 운반하는 임무를 맡게', '된다. 그 데이터에는 거대 기업', \"'울프 코퍼레이션'의 시민에 대한\", '악랄한 지배를 폭로하는 정보가 담겨', '있었다. 그녀는 데이터를 받아', '목적지로 향한다.', '제2장: 울프 코퍼레이션의 함정', \"미코는 목적지인 술집 '할머니의\", \"집'으로 향하는 길에 울프\", '코퍼레이션의 요원들에게 쫓기게', \"된다. 그들은 '빨간 망토'라는\", '데이터 카우리아에 대한 소문을 듣고', '데이터를 탈취하려 했다. 미코는', '교묘하게 요원들을 흩뿌리고 술집에', '도착한다.', '제3장: 배신과 재회', \"술집 '할머니의 집'에서 미코는\", '데이터를 받을 사람인 료를 기다리고', '있었다. 료는 그녀의 어릴 적', '적 친구이자 그 역시 울프', '코퍼레이션과 싸우는 해커 집단의', '일원이었다. 하지만 료는 미코에게', '배신감을 느꼈고, 그녀가 데이터', '카우리아에 몸을 던진 것에 화가', '났다.', '그럼에도 불구하고 미코는 료에게', '데이터를 건네며 울프 코퍼레이션에', '대한 반격을 믿기로 한다. 두', '두 사람은 함께 울프 코퍼레이션의', '음모를 밝혀내고 시민들을 구하기로', '결심한다.', '제4장: 울프 코퍼레이션의 붕괴', '미코와 료는 해커 집단과 함께 울프', '코퍼레이션에 대한 최후의 결전을', '벌인다. 능숙한 해킹 기술과 신체', '능력으로 그들은 울프 코퍼레이션의', '보안을 차례로 뚫어 나간다. 그', '그 과정에서 미코는 울프', '코퍼레이션이 어머니의 병에 관여하고', '있다는 사실을 알게 된다. 그녀는', '분노에 휩싸여 울프코퍼레이션에 대한', '복수를 다짐한다.', '제5장: 결전의 순간', '미코와 료는 마침내 울프', '코퍼레이션의 최상층에 도착해', 'CEO인 교활한 울프 박사와', '대면한다. 울프 박사는 시민을', '지배하려는 사악한 야망을 드러내며', '자신의 압도적인 힘을 과시한다.', '하지만 미코와 료는 서로를 도와가며', '울프 박사와 싸우고 그의 약점을', '찾아낸다.', '제6장: 진실의 해방', '미코는 울프 박사의 약점을 파고들어', '그를 쓰러뜨리는데 성공한다. 그리고', '해커 집단과 함께 울프 코퍼레이션의', '악행을 세상에 공개하고 시민들을', '해방시킨다. 이 승리로 미코의', '어머니의 치료법도 찾아내고, 그녀의', '병은 완치된다.', '제7장: 새로운 시작', '울프 코퍼레이션이 무너진 후,', '미코와 료는 서로의 과거를 용서하고', '다시 우정을 회복한다. 미코는', '데이터카우리아를 그만두고 료와 함께', '새로운 길을 걷기 시작한다. 그들은', '스스로의 힘으로 미래의 네오 도쿄를', '더 나은 도시로 바꾸어 나갈 것을', '다짐한다. 이것은 미코와 료,', '그리고 전뇌 빨간 망토의 새로운', '모험의 시작이었다.', '결말']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80543f83bbfc4f3c810b91400f7a69f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9ac1e6583b4f94aaff657d1474f981"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd2a1397f4d1481f87013d9a1b0b0c85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"305781a661b347a7879e65c2fe8fd488"}},"metadata":{}},{"name":"stdout","text":"[\"제목: '전뇌 빨간 망토'\", '제1장: 데이터 프론트', '밤이 되면 반짝이는 네오 도쿄. 고층 빌딩이 늘어서고, 네온사인이 거리를 수놓는다. 그 거리에서 빨간 두건을 쓴 소녀 미코는 불법 데이터 카우리아를 운반하는 배달원으로 일하고 있었다. 그녀는 어머니가 병에 걸려 치료비를 벌기 위해 데이터카우리아에 몸을 던지고 있었다.', \"그러던 어느 날, 미코는 중요한 데이터를 운반하는 임무를 맡게 된다. 그 데이터에는 거대 기업 '울프 코퍼레이션'의 시민에 대한 악랄한 지배를 폭로하는 정보가 담겨 있었다. 그녀는 데이터를 받아 목적지로 향한다.\", '제2장: 울프 코퍼레이션의 함정', \"미코는 목적지인 술집 '할머니의 집'으로 향하는 길에 울프 코퍼레이션의 요원들에게 쫓기게 된다. 그들은 '빨간 망토'라는 데이터 카우리아에 대한 소문을 듣고 데이터를 탈취하려 했다. 미코는 교묘하게 요원들을 흩뿌리고 술집에 도착한다.\", '제3장: 배신과 재회', \"술집 '할머니의 집'에서 미코는 데이터를 받을 사람인 료를 기다리고 있었다. 료는 그녀의 어릴 적 친구이자 그 역시 울프 코퍼레이션과 싸우는 해커 집단의 일원이었다. 하지만 료는 미코에게 배신감을 느꼈고, 그녀가 데이터 카우리아에 몸을 던진 것에 화가 났다.\", '그럼에도 불구하고 미코는 료에게 데이터를 건네며 울프 코퍼레이션에 대한 반격을 믿기로 한다. 두 사람은 함께 울프 코퍼레이션의 음모를 밝혀내고 시민들을 구하기로 결심한다.', '제4장: 울프 코퍼레이션의 붕괴', '미코와 료는 해커 집단과 함께 울프 코퍼레이션에 대한 최후의 결전을 벌인다. 능숙한 해킹 기술과 신체 능력으로 그들은 울프 코퍼레이션의 보안을 차례로 뚫어 나간다. 그 과정에서 미코는 울프 코퍼레이션이 어머니의 병에 관여하고 있다는 사실을 알게 된다. 그녀는 분노에 휩싸여 울프코퍼레이션에 대한 복수를 다짐한다.', '제5장: 결전의 순간', '미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신의 압도적인 힘을 과시한다. 하지만 미코와 료는 서로를 도와가며 울프 박사와 싸우고 그의 약점을 찾아낸다.', '제6장: 진실의 해방', '미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다. 그리고 해커 집단과 함께 울프 코퍼레이션의 악행을 세상에 공개하고 시민들을 해방시킨다. 이 승리로 미코의 어머니의 치료법도 찾아내고, 그녀의 병은 완치된다.', '제7장: 새로운 시작', '울프 코퍼레이션이 무너진 후, 미코와 료는 서로의 과거를 용서하고 다시 우정을 회복한다. 미코는 데이터카우리아를 그만두고 료와 함께 새로운 길을 걷기 시작한다. 그들은 스스로의 힘으로 미래의 네오 도쿄를 더 나은 도시로 바꾸어 나갈 것을 다짐한다. 이것은 미코와 료, 그리고 전뇌 빨간 망토의 새로운 모험의 시작이었다.', '결말']\n[Document(page_content='Foo'), Document(page_content='Some intro text about Foo.  \\nBar main section Bar subsection 1 Bar subsection 2', metadata={'Header 1': 'Foo'}), Document(page_content='Some intro text about Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}), Document(page_content='Some text about the first subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}), Document(page_content='Some text about the second subtopic of Bar.', metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}), Document(page_content='Baz', metadata={'Header 1': 'Foo'}), Document(page_content='Some text about Baz', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}), Document(page_content='Some concluding text about Foo', metadata={'Header 1': 'Foo'})]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Document\n\nCreate a document. You can use document directly to create a document, or you can import another type of file to create a document automatically. In this case, we will use the split we used earlier. Creating a document like this will make it easier to use later in vector index. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom langchain.document_loaders import PyPDFLoader, DataFrameLoader, BSHTMLLoader\nfrom langchain.schema import Document\n\n\n# document 만들기\nmy_page = Document(\npage_content=\"이 문서는 제 문서입니다. 다른 곳에서 수집한 텍스트로 가득합니다.\",\nmetadata={'explain': 'The LangChain Papers'})\nprint(my_page)\n\n\n# 다중 문서 만들기\nmy_list = [\n\"Hi there!\",\n\"Oh, hello!\",\n\"What's your name?\",\n\"My friends call me World\",\n\"Hello World!\"\n]\n\nmy_pages = [Document(page_content = i) for i in my_list]\nprint(my_pages)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:50.106690Z","iopub.execute_input":"2023-11-13T07:43:50.107954Z","iopub.status.idle":"2023-11-13T07:43:50.615461Z","shell.execute_reply.started":"2023-11-13T07:43:50.107886Z","shell.execute_reply":"2023-11-13T07:43:50.614284Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"page_content='이 문서는 제 문서입니다. 다른 곳에서 수집한 텍스트로 가득합니다.' metadata={'explain': 'The LangChain Papers'}\n[Document(page_content='Hi there!'), Document(page_content='Oh, hello!'), Document(page_content=\"What's your name?\"), Document(page_content='My friends call me World'), Document(page_content='Hello World!')]\n","output_type":"stream"}]},{"cell_type":"code","source":"# PyPDF Loader # pip install pypdf\n\nloader = PyPDFLoader(\"/kaggle/input/langchain-tutorial/field-guide-to-data-science.pdf\")\npages = loader.load_and_split()\nprint(pages[:2])","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:50.617372Z","iopub.execute_input":"2023-11-13T07:43:50.620619Z","iopub.status.idle":"2023-11-13T07:43:59.426258Z","shell.execute_reply.started":"2023-11-13T07:43:50.620563Z","shell.execute_reply":"2023-11-13T07:43:59.425000Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[Document(page_content='DATA SCIENCEtoTHE\\nFIEL D GUIDE\\n    \\n \\nSECOND  \\nEDITION\\n© COPYRIGHT 2015 BOOZ ALLEN HAMILTON INC. ALL RIGHTS RESERVED.', metadata={'source': '/kaggle/input/langchain-tutorial/field-guide-to-data-science.pdf', 'page': 1}), Document(page_content='FOREWORD\\nData Science touches every aspect of our lives on a \\ndaily basis. When we visit the doctor, drive our cars, \\nget on an airplane, or shop for services, Data Science \\nis changing the way we interact with and explore  \\nour world.  \\nOur world is now measured, \\nmapped, and recorded in digital \\nbits. Entire lives, from birth to \\ndeath, are now catalogued in \\nthe digital realm. These data, \\noriginating from such diverse \\nsources as connected vehicles, \\nunderwater microscopic cameras, \\nand photos we post to social \\nmedia, have propelled us into \\nthe greatest age of discovery \\nhumanity has ever known. It is \\nthrough Data Science that we \\nare unlocking the secrets hidden \\nwithin these data. We are making \\ndiscoveries that will forever \\nchange how we live and interact \\nwith the world around us. \\nThe impact of these changes \\nis having a profound effect on \\nhumanity. We have propelled \\nourselves into this age of \\ndiscovery through our incremental \\ntechnological improvements. \\nData Science has become the \\ncatalyzing force behind our next \\nevolutionary leap. Our own \\nevolution is now inextricably \\nlinked to that of computers. The \\nway we live our lives and the skills \\nthat are important to our very \\nexistence are directly dependent \\nupon the functions Data Science \\ncan achieve on our behalf.   As we move into this new \\nfuture, it is clearer than ever, that \\nbusinesses must adjust to these \\nchanges or risk being left behind. \\nFrom influencing retail markets, \\nto setting public health and safety \\npolicies, or to addressing social \\nunrest, organizations of all types \\nare generating value through  \\nData Science. Data is our new \\ncurrency and Data Science is  \\nthe mechanism by which we tap \\ninto it. \\nData Science is an auspicious and \\nprofound way of applying our \\ncuriosity and technical tradecraft \\nto solve humanity’s toughest \\nchallenges. The growing power, \\nimportance, and responsibility \\nof applying Data Science \\nmethodologies to these challenges \\nis unimaginable. Our own \\nbiases and assumptions can have \\nprofound outcomes on business, \\nnational security, and our daily \\nlives. A new class of practitioners \\nand leaders are needed to navigate \\nthis new future. Data Scientists \\nare our guides on this journey as \\nthey are creating radical new ways \\nof thinking about data and the \\nworld around us.  \\n \\nWe want to share our passion for  Data Science  and start a \\nconversation with you. This is a journey worth taking.  ››', metadata={'source': '/kaggle/input/langchain-tutorial/field-guide-to-data-science.pdf', 'page': 3})]\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame Loader\ndf = pd.read_csv(\"/kaggle/input/langchain-tutorial/mlb_teams_2012.csv\")\nloader = DataFrameLoader(df, page_content_column=\"Team\")\npages = loader.load_and_split()\nprint(pages[:5])","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:59.427549Z","iopub.execute_input":"2023-11-13T07:43:59.427941Z","iopub.status.idle":"2023-11-13T07:43:59.457842Z","shell.execute_reply.started":"2023-11-13T07:43:59.427885Z","shell.execute_reply":"2023-11-13T07:43:59.456572Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[Document(page_content='Nationals', metadata={' \"Payroll (millions)\"': 81.34, ' \"Wins\"': 98}), Document(page_content='Reds', metadata={' \"Payroll (millions)\"': 82.2, ' \"Wins\"': 97}), Document(page_content='Yankees', metadata={' \"Payroll (millions)\"': 197.96, ' \"Wins\"': 95}), Document(page_content='Giants', metadata={' \"Payroll (millions)\"': 117.62, ' \"Wins\"': 94}), Document(page_content='Braves', metadata={' \"Payroll (millions)\"': 83.31, ' \"Wins\"': 94})]\n","output_type":"stream"}]},{"cell_type":"code","source":"# BS4 HTML Loader\nloader = BSHTMLLoader(\"/kaggle/input/langchain-tutorial/fake-content.html\")\npages = loader.load_and_split()\nprint(pages[:5])","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:59.459076Z","iopub.execute_input":"2023-11-13T07:43:59.459731Z","iopub.status.idle":"2023-11-13T07:43:59.655162Z","shell.execute_reply.started":"2023-11-13T07:43:59.459696Z","shell.execute_reply":"2023-11-13T07:43:59.653991Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[Document(page_content='Test Title\\n\\n\\nMy First Heading\\nMy first paragraph.', metadata={'source': '/kaggle/input/langchain-tutorial/fake-content.html', 'title': 'Test Title'})]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### vector index\n\nThe vector index is the most basic element of RAG. They are used to find the most similar sentences to a query to get a more accurate answer. This example uses a library called Faiss to show how it works. Metadata can provide additional information about what a document is about. This can lead to finding more accurate documents. similarity_search compares cosine similarity and uses k documents. ","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.vectorstores.faiss import FAISS\n\n# 데이터 준비\nwith open('/kaggle/input/langchain-tutorial/akazukin_all.txt', encoding='utf-8') as f:\n    akazukin_all = f.read()\n\n# 청크 분할\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=300,  # 청크의 최대 문자 수\n    chunk_overlap=20,  # 최대 오버랩 문자 수\n)\ntexts = text_splitter.split_text(akazukin_all)\n\n# 확인\nprint(len(texts))\nfor text in texts:\n    print(text[:10], \":\", len(text))\n\n# 메타데이터 준비\nmetadatas = [\n    {\"source\": \"1장\"},\n    {\"source\": \"2장\"},\n    {\"source\": \"3장\"},\n    {\"source\": \"4장\"},\n    {\"source\": \"5~6장\"},\n    {\"source\": \"7장\"}\n]\n\n# Faiss 벡터 인덱스 생성\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\ndocsearch = FAISS.from_texts(texts=texts,  # 청크 배열\n                             embedding=embeddings,  # 임베딩\n                             metadatas=metadatas  # 메타데이터\n                             )\n\nquery=\"미코의 소꿉친구 이름은?\"\ndocs = docsearch.similarity_search(query, k=3)\n\nfor i in docs:\n    print(i.page_content)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-13T07:43:59.656490Z","iopub.execute_input":"2023-11-13T07:43:59.656839Z","iopub.status.idle":"2023-11-13T07:44:00.567050Z","shell.execute_reply.started":"2023-11-13T07:43:59.656809Z","shell.execute_reply":"2023-11-13T07:44:00.565746Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"6\n제목: '전뇌 빨간 : 299\n제2장: 울프 코퍼 : 162\n제3장: 배신과 재 : 273\n제4장: 울프 코퍼 : 206\n제5장: 결전의 순 : 294\n제7장: 새로운 시 : 195\n제2장: 울프 코퍼레이션의 함정\n\n미코는 목적지인 술집 '할머니의 집'으로 향하는 길에 울프 코퍼레이션의 요원들에게 쫓기게 된다. 그들은 '빨간 망토'라는 데이터 카우리아에 대한 소문을 듣고 데이터를 탈취하려 했다. 미코는 교묘하게 요원들을 흩뿌리고 술집에 도착한다.\n\n제3장: 배신과 재회\n제5장: 결전의 순간\n\n미코와 료는 마침내 울프 코퍼레이션의 최상층에 도착해 CEO인 교활한 울프 박사와 대면한다. 울프 박사는 시민을 지배하려는 사악한 야망을 드러내며 자신의 압도적인 힘을 과시한다. 하지만 미코와 료는 서로를 도와가며 울프 박사와 싸우고 그의 약점을 찾아낸다.\n\n제6장: 진실의 해방\n\n미코는 울프 박사의 약점을 파고들어 그를 쓰러뜨리는데 성공한다. 그리고 해커 집단과 함께 울프 코퍼레이션의 악행을 세상에 공개하고 시민들을 해방시킨다. 이 승리로 미코의 어머니의 치료법도 찾아내고, 그녀의 병은 완치된다.\n제7장: 새로운 시작\n\n울프 코퍼레이션이 무너진 후, 미코와 료는 서로의 과거를 용서하고 다시 우정을 회복한다. 미코는 데이터카우리아를 그만두고 료와 함께 새로운 길을 걷기 시작한다. 그들은 스스로의 힘으로 미래의 네오 도쿄를 더 나은 도시로 바꾸어 나갈 것을 다짐한다. 이것은 미코와 료, 그리고 전뇌 빨간 망토의 새로운 모험의 시작이었다.\n\n결말\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Part.2에서 계속 ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}