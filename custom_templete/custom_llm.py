import os
from pydantic import Field
from typing import List, Mapping, Optional, Any
from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens
from langchain.schema import Generation, LLMResult

class CustomLLM(LLM):
    """
    generator를 정의해서 사용 가능하게 끔 코드 생성
    """
    model_folder_path: str = Field(None, alias='model_folder_path')
    model_name: str = Field(None, alias='model_name')
    backend: Optional[str] = 'llama'
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.1
    top_k: Optional[int] = 40
    max_tokens: Optional[int] = 200
    streaming: bool = False


    def __init__(self, model_folder_path, model_name, **kwargs):
        super(CustomLLM, self).__init__()
        self.model_folder_path: str = model_folder_path
        self.model_name = model_name
        self.generator = (self.model_folder_path, self.model_name)

    @property
    def _get_model_default_parameters(self):
        return {
            "max_tokens": self.max_tokens,
            "top_k": self.top_k,
            "top_p": self.top_p,
            "temperature": self.temperature,
        }

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {
            'model_name': self.model_name,
            'model_path': self.model_folder_path,
            'model_parameters': self._get_model_default_parameters
        }

    @property
    def _llm_type(self) -> str:
        return 'llama'

    def _call(self,
              prompt: str,
              stop: Optional[List[str]] = None,
              **kwargs) -> LLMResult:
        """
        Args:
            prompt: The prompt to pass into the model.
            stop: A list of strings to stop generation when encountered

        Returns:
            The string generated by the model        
        """

        params = {
            **self._get_model_default_parameters,
            **kwargs
        }
        if self.streaming:
            raise ValueError("`streaming` option currently unsupported.")

        text_generations: List[str] = []
        response = self.generator.generate(prompt, **params)

        if stop:
            response = enforce_stop_tokens(response, stop)

        text_generations.append(response)

        return LLMResult(generations=[[Generation(text=text)] for text in text_generations])
