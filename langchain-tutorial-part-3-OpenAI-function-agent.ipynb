{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hypro2/hands-on-langchain/blob/main/langchain-tutorial-part-3-OpenAI-function-agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습 자료 출처 :\n",
        "https://github.com/hwchase17/ai-engineer\n",
        "\n"
      ],
      "metadata": {
        "id": "BgGsP9UNtrtk"
      },
      "id": "BgGsP9UNtrtk"
    },
    {
      "cell_type": "markdown",
      "id": "16de7336",
      "metadata": {
        "id": "16de7336"
      },
      "source": [
        "# OpenAI Function Calling In LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain==0.1.0\n",
        "!pip install -q openai\n",
        "!pip install -q -U langchain-openai"
      ],
      "metadata": {
        "id": "yVCXqAZkL6lk"
      },
      "id": "yVCXqAZkL6lk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "except:\n",
        "    openai_api_key = os.getenv('OPENAI_API_KEY', getpass.getpass(\"입력해주세요.\"))"
      ],
      "metadata": {
        "id": "NwQgHa6OMVMY"
      },
      "id": "NwQgHa6OMVMY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f6e3fb26-4082-4651-ac37-7a776de8dd08",
      "metadata": {
        "id": "f6e3fb26-4082-4651-ac37-7a776de8dd08"
      },
      "source": [
        "## Pydantic to OpenAI function definition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "196549ec-683b-4c1c-ba71-4ac49c377e2d",
      "metadata": {
        "id": "196549ec-683b-4c1c-ba71-4ac49c377e2d"
      },
      "source": [
        "구성 함수인 'WeatherSearch'에 대한 Pydantic 데이터 클래스를 정의해 보겠습니다.\n",
        "\n",
        "이 함수는 구성된 날씨 API 호출에 필요한 매개변수 집합을 반환합니다.\n",
        "\n",
        "개발할 때, 딕셔너리를 통한 형식보다 Pydantic 데이터 클래스가 관리하기 편리합니다.\n",
        "\n",
        "현재 버전에서는 pydantic.v1을 기준으로 작성되고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc5794db",
      "metadata": {
        "id": "bc5794db"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic.v1 import BaseModel, Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752ae724-549b-45f2-89d4-599682628791",
      "metadata": {
        "id": "752ae724-549b-45f2-89d4-599682628791"
      },
      "outputs": [],
      "source": [
        "# 구 버전\n",
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
        "\n",
        "# 신규 버전\n",
        "from langchain_core.utils.function_calling import convert_to_openai_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065b132b-f476-41d6-8da8-27169a7cdfe3",
      "metadata": {
        "id": "065b132b-f476-41d6-8da8-27169a7cdfe3"
      },
      "outputs": [],
      "source": [
        "class WeatherSearch(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\" # ```은 description에 해당합니다.\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\") # 변수는 key, 필드는 value가 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e9c7fc3-c922-4936-8a70-7da380ed8690",
      "metadata": {
        "id": "3e9c7fc3-c922-4936-8a70-7da380ed8690"
      },
      "source": [
        "Openai를 이용하기 위해서는 pydantic 구조를 바로 쓰지는 못하고 변환이 필요하는 과정이 필요합니다.\n",
        "\n",
        "convert_pydantic_to_openai_function을 이용해서 pydantic구조를 openai function구조로 변경해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WeatherSearch.schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6ljN1-8QO6g",
        "outputId": "c25219cc-1545-47c8-f425-aa30a046a52e"
      },
      "id": "D6ljN1-8QO6g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'WeatherSearch',\n",
              " 'description': 'Call this with an airport code to get the weather at that airport',\n",
              " 'type': 'object',\n",
              " 'properties': {'airport_code': {'title': 'Airport Code',\n",
              "   'description': 'airport code to get weather for',\n",
              "   'type': 'string'}},\n",
              " 'required': ['airport_code']}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b254d0f-16c4-4af0-940d-059742ef074f",
      "metadata": {
        "id": "9b254d0f-16c4-4af0-940d-059742ef074f",
        "outputId": "e0816719-6453-4e4c-c3ec-dd1b1185c097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'WeatherSearch',\n",
              " 'description': 'Call this with an airport code to get the weather at that airport',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'airport_code': {'description': 'airport code to get weather for',\n",
              "    'type': 'string'}},\n",
              "  'required': ['airport_code']}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "weather_function = convert_to_openai_function(WeatherSearch)\n",
        "weather_function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI의 function calling에서 description은 필수 사항입니다.\n",
        "\n",
        "pydentic 데이터 클래스에서는 \"\"\" \"\"\" 속 설명이 description에 해당합니다."
      ],
      "metadata": {
        "id": "p_fmjmek0Vt3"
      },
      "id": "p_fmjmek0Vt3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73ce5b0-6394-4dbb-a6e7-2ef91fece27b",
      "metadata": {
        "id": "d73ce5b0-6394-4dbb-a6e7-2ef91fece27b",
        "outputId": "461a9889-7df6-4eef-d4bb-6514692b287a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'WeatherSearch1',\n",
              " 'description': '',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'airport_code': {'description': 'airport code to get weather for',\n",
              "    'type': 'string'}},\n",
              "  'required': ['airport_code']}}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "# We require it have a description\n",
        "class WeatherSearch1(BaseModel):\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")\n",
        "\n",
        "convert_pydantic_to_openai_function(WeatherSearch1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai  import ChatOpenAI"
      ],
      "metadata": {
        "id": "gAU8b-yoscXA"
      },
      "id": "gAU8b-yoscXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353f0e47-b147-4da4-ace3-80db6270c1ab",
      "metadata": {
        "id": "353f0e47-b147-4da4-ace3-80db6270c1ab",
        "outputId": "20f05d48-eb9d-4ab9-c0a9-9accdfce88ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "model.invoke(\"What is the weather in San Francisco right now?\",\n",
        "             functions=[weather_function])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1e3f97-cc06-4adf-8ee3-a3065e15c9c5",
      "metadata": {
        "id": "db1e3f97-cc06-4adf-8ee3-a3065e15c9c5"
      },
      "outputs": [],
      "source": [
        "model_with_function = model.bind(functions=[weather_function])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232c231a-1ec9-42f4-943b-ea363d0f9dc6",
      "metadata": {
        "id": "232c231a-1ec9-42f4-943b-ea363d0f9dc6",
        "outputId": "d31ef4cc-5f4c-4519-e7a1-7d51c14e3826",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model_with_function.invoke(\"What is the weather in San Francisco right now?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d17ffbee-23c7-4f33-a7cd-209654fa9b8e",
      "metadata": {
        "id": "d17ffbee-23c7-4f33-a7cd-209654fa9b8e"
      },
      "source": [
        "## Forcing it to use a function\n",
        "\n",
        "bind을 통해 function을 넣어주면 모델을 불러올때 함수를 사용하도록 할  수있습니다.\n",
        "\n",
        "function call을 지정해줘서 사용하도록 강제할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6cc8ed8-6039-426e-983f-46ec66a42cb6",
      "metadata": {
        "id": "c6cc8ed8-6039-426e-983f-46ec66a42cb6"
      },
      "outputs": [],
      "source": [
        "model_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f97f97-4fb2-4d47-9ca3-ec8e58192b45",
      "metadata": {
        "id": "79f97f97-4fb2-4d47-9ca3-ec8e58192b45",
        "outputId": "5ff3c60c-f752-4487-a0a5-408128a62e59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model_forced_function.invoke(\"What is the weather in San Francisco right now?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b689e85-f47e-4cd9-9fe8-1db07cf04713",
      "metadata": {
        "id": "4b689e85-f47e-4cd9-9fe8-1db07cf04713"
      },
      "source": [
        "## Using in a chain\n",
        "\n",
        "평소처럼 체인에서 작동하도록 바인딩된 이 모델을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "jxbGEEMgsfEC"
      },
      "id": "jxbGEEMgsfEC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd7d521-3c2b-48e3-80a4-67e6dd477568",
      "metadata": {
        "id": "7cd7d521-3c2b-48e3-80a4-67e6dd477568"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helfpul assistant\"),\n",
        "        (\"user\", \"{input}\")\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16864122-2218-47e9-a340-bb62ec05ae3b",
      "metadata": {
        "id": "16864122-2218-47e9-a340-bb62ec05ae3b"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model_forced_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb7f985-ac66-4bae-9af4-126cb2146c69",
      "metadata": {
        "id": "6bb7f985-ac66-4bae-9af4-126cb2146c69",
        "outputId": "33e4f8f3-0067-4bd1-8ce9-d138cdca9321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"What is the weather in San Francisco right now?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0984ce8b",
      "metadata": {
        "id": "0984ce8b"
      },
      "source": [
        "## Using multiple functions\n",
        "\n",
        "이보다 더 좋은 방법은 함수 집합을 전달하고 LLM이 질문 컨텍스트에 따라 어떤 함수를 사용할지 결정하도록 하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae750073",
      "metadata": {
        "id": "ae750073"
      },
      "outputs": [],
      "source": [
        "class ArtistSearch(BaseModel):\n",
        "    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n",
        "\n",
        "    artist_name: str = Field(description=\"name of artist to look up\")\n",
        "    n: int = Field(description=\"number of results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10807cbf",
      "metadata": {
        "id": "10807cbf",
        "outputId": "afa84c70-5ad7-43f8-93c8-33093592ff42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'ArtistSearch',\n",
              " 'description': 'Call this to get the names of songs by a particular artist',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'artist_name': {'description': 'name of artist to look up',\n",
              "    'type': 'string'},\n",
              "   'n': {'description': 'number of results', 'type': 'integer'}},\n",
              "  'required': ['artist_name', 'n']}}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "convert_to_openai_function(ArtistSearch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d22d51b1",
      "metadata": {
        "id": "d22d51b1"
      },
      "outputs": [],
      "source": [
        "model = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "functions = [\n",
        "    convert_to_openai_function(WeatherSearch),\n",
        "    convert_to_openai_function(ArtistSearch),\n",
        "]\n",
        "\n",
        "model_with_functions = model.bind(functions=functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4329db11",
      "metadata": {
        "id": "4329db11",
        "outputId": "421908d6-0356-459b-a9f8-c9bcfa38c4a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n\"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model_with_functions.invoke(\"What is the weather in SF?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6a4566-8325-4180-963a-62c5fbf0a3b7",
      "metadata": {
        "id": "ff6a4566-8325-4180-963a-62c5fbf0a3b7",
        "outputId": "453c4158-e5ac-4e8e-c7be-89743bca569c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n\"artist_name\": \"Taylor Swift\",\\n\"n\": 3\\n}', 'name': 'ArtistSearch'}})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model_with_functions.invoke(\"What are three songs by Taylor swift?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R36JM-sf9EyG"
      },
      "id": "R36JM-sf9EyG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tagging and Extraction Using OpenAI functions\n",
        "\n",
        "## 태깅\n",
        "이전에는 함수(Function)를 사용하여 자연어 입력에서 특정 API 매개변수를 추출했습니다.\n",
        "\n",
        "여기서는 함수(Function)가 매우 유연하다는 것을 보여줍니다.\n",
        "\n",
        "함수(Function)를 사용하여 텍스트에 특정 정보를 쉽게 태그할 수 있습니다."
      ],
      "metadata": {
        "id": "dRmjIpjqNyhO"
      },
      "id": "dRmjIpjqNyhO"
    },
    {
      "cell_type": "code",
      "source": [
        "class Tagging(BaseModel):\n",
        "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
        "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
        "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")"
      ],
      "metadata": {
        "id": "GVaLRUeVNyLk"
      },
      "id": "GVaLRUeVNyLk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_openai_function(Tagging)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvrJdhXDNoSS",
        "outputId": "0e7712b0-d624-40a7-df35-f5df16085f0f"
      },
      "id": "SvrJdhXDNoSS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Tagging',\n",
              " 'description': 'Tag the piece of text with particular info.',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'sentiment': {'description': 'sentiment of text, should be `pos`, `neg`, or `neutral`',\n",
              "    'type': 'string'},\n",
              "   'language': {'description': 'language of text (should be ISO 639-1 code)',\n",
              "    'type': 'string'}},\n",
              "  'required': ['sentiment', 'language']}}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai  import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0,openai_api_key=openai_api_key)\n",
        "tagging_functions = [convert_to_openai_function(Tagging)]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Think carefully, and then tag the text as instructed.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# We pass function_call to MAKE it call this function\n",
        "model_with_functions = model.bind(functions=tagging_functions, function_call={\"name\":\"Tagging\"})\n",
        "\n",
        "tagging_chain = prompt | model_with_functions\n",
        "\n",
        "tagging_chain.invoke({\"input\": \"I love LangChain\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zzS6MH9N9cn",
        "outputId": "c80ff82c-cc15-4dfb-9cc7-618e913c0128"
      },
      "id": "-zzS6MH9N9cn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"sentiment\": \"pos\",\\n  \"language\": \"en\"\\n}', 'name': 'Tagging'}})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
      ],
      "metadata": {
        "id": "G5T2xb9Nsilx"
      },
      "id": "G5T2xb9Nsilx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()\n",
        "\n",
        "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"}) # 난 음식을 좋아하지 않아요"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFb_oBUrOICK",
        "outputId": "bd4c97a4-adce-4120-b837-c4323927a804"
      },
      "id": "LFb_oBUrOICK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'neg', 'language': 'it'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추출\n",
        "추출은 태깅과 유사하지만 여러 정보를 추출하는 데 사용됩니다.\n",
        "\n",
        "Pydantic 데이터 클래스를 결합해서 함수(Function)를 복합적으로 만들 수 있습니다."
      ],
      "metadata": {
        "id": "QMES_yK3OaTM"
      },
      "id": "QMES_yK3OaTM"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "    name: str = Field(description=\"person's name\")\n",
        "    age: Optional[int] = Field(description=\"person's age\")\n",
        "\n",
        "class Information(BaseModel):\n",
        "    \"\"\"Information to extract.\"\"\"\n",
        "    people: List[Person] = Field(description=\"List of info about people\")"
      ],
      "metadata": {
        "id": "DXt8YmOMOceG"
      },
      "id": "DXt8YmOMOceG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(Information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhs31tx5PQtI",
        "outputId": "a66b0ca9-6e79-47a6-8b13-9e1bfe9a103c"
      },
      "id": "bhs31tx5PQtI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Information',\n",
              " 'description': 'Information to extract.',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'people': {'description': 'List of info about people',\n",
              "    'type': 'array',\n",
              "    'items': {'description': 'Information about a person.',\n",
              "     'type': 'object',\n",
              "     'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
              "      'age': {'description': \"person's age\", 'type': 'integer'}},\n",
              "     'required': ['name']}}},\n",
              "  'required': ['people']}}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_openai_function(Information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPWGBRojV0xP",
        "outputId": "b16d4e90-5570-49c3-bd77-d39ab52e3937"
      },
      "id": "UPWGBRojV0xP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Information',\n",
              " 'description': 'Information to extract.',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'people': {'description': 'List of info about people',\n",
              "    'type': 'array',\n",
              "    'items': {'description': 'Information about a person.',\n",
              "     'type': 'object',\n",
              "     'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
              "      'age': {'description': \"person's age\", 'type': 'integer'}},\n",
              "     'required': ['name']}}},\n",
              "  'required': ['people']}}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "FZsUO2xXslkB"
      },
      "id": "FZsUO2xXslkB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
        "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\":\"Information\"})\n",
        "\n",
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")\n",
        "extraction_chain.invoke({\"input\": \"Joe is 30. Joe's mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7RCsxczOjGa",
        "outputId": "11744189-433e-4242-a61d-20b074f51696"
      },
      "id": "h7RCsxczOjGa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Joe', 'age': 30}, {'name': 'Martha'}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nA5U-_8XGtim"
      },
      "id": "nA5U-_8XGtim",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Tool\n",
        "\n",
        "이전에는 OpenAI 모델에 함수(Function)를 제공하는 방법을 보여드렸습니다.\n",
        "\n",
        "함수(Function)는 작업(예: 추출, 태그 지정)이나 **API에 필요한 출력 스키마**를 지정하는 데 사용할 수 있습니다.\n",
        "\n",
        "도구는 보다 일반적인 개념으로 LLM 에이전트의 중심 개념입니다.\n",
        "\n",
        "종종 도구는 LLM이 쉽게 액세스할 수 있도록 API를 래핑합니다.\n",
        "\n",
        "_\n",
        "\n",
        "**도구는 LLM을 이용해서 도구에 들어갈 Input을 찾는 작업!**"
      ],
      "metadata": {
        "id": "BrD9-20FgbNG"
      },
      "id": "BrD9-20FgbNG"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool"
      ],
      "metadata": {
        "id": "j31E4SYPsn4b"
      },
      "id": "j31E4SYPsn4b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def search(query: str) -> str:\n",
        "    \"\"\"Search for the weather online.\"\"\"\n",
        "    return \"result\""
      ],
      "metadata": {
        "id": "1HDFUpQ8OkMF"
      },
      "id": "1HDFUpQ8OkMF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nzNSYqtRgs_H",
        "outputId": "97a8f414-8cc5-4012-92c3-fb49d72c49c5"
      },
      "id": "nzNSYqtRgs_H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'search'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r_hyxj5rgt5-",
        "outputId": "a24158c7-b1af-4c69-8684-f0071f1fd59a"
      },
      "id": "r_hyxj5rgt5-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'search(query: str) -> str - Search for the weather online.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j4VoRI_gvAo",
        "outputId": "14145246-739d-477c-a5f8-2dc0e38bcc70"
      },
      "id": "_j4VoRI_gvAo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'title': 'Query', 'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "도구에 사용할 Input을 LLM을 통해서 찾아 정의하는 것!!\n",
        "\n",
        "description과 search.args을 통해서 정의"
      ],
      "metadata": {
        "id": "SFOQS4J7gY9v"
      },
      "id": "SFOQS4J7gY9v"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic.v1 import BaseModel, Field\n",
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "class SearchInput(BaseModel):\n",
        "    query: str = Field(description=\"Thing to search for\")\n",
        "\n",
        "@tool(args_schema=SearchInput)\n",
        "def search(query: str) -> str:\n",
        "    \"\"\"Search for the weather online.\"\"\"\n",
        "    return \"result\"\n",
        "\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "\n",
        "# format_tool_to_openai_function(search)\n",
        "convert_to_openai_function(search)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzCMMFEPgwB2",
        "outputId": "521314d6-1a7c-4bef-c481-880a3f8ae421"
      },
      "id": "pzCMMFEPgwB2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'search',\n",
              " 'description': 'search(query: str) -> str - Search for the weather online.',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'query': {'description': 'Thing to search for',\n",
              "    'type': 'string'}},\n",
              "  'required': ['query']}}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.run(\"sf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bTbM4906g3od",
        "outputId": "253f917f-7805-4b9f-e448-a856725b4b05"
      },
      "id": "bTbM4906g3od",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'result'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 온도 도구\n",
        "(경도,위도)만 있으면 온도 찾아주는 툴\n",
        "\n",
        "우리가 필요한 것은 경도와 위도 이다. 온도는 도구에서 찾아줄 것이기 때문에\n"
      ],
      "metadata": {
        "id": "KRTCnq4z2lLB"
      },
      "id": "KRTCnq4z2lLB"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pydantic.v1 import BaseModel, Field\n",
        "import datetime\n",
        "\n",
        "# Define the input schema\n",
        "class OpenMeteoInput(BaseModel):\n",
        "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
        "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
        "\n",
        "@tool(args_schema=OpenMeteoInput)\n",
        "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
        "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "\n",
        "    # Parameters for the request\n",
        "    params = {\n",
        "        'latitude': latitude,\n",
        "        'longitude': longitude,\n",
        "        'hourly': 'temperature_2m',\n",
        "        'forecast_days': 1,\n",
        "    }\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "    else:\n",
        "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
        "\n",
        "    current_utc_time = datetime.datetime.utcnow()\n",
        "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
        "    temperature_list = results['hourly']['temperature_2m']\n",
        "\n",
        "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
        "    current_temperature = temperature_list[closest_time_index]\n",
        "\n",
        "    return f'The current temperature is {current_temperature}°C'"
      ],
      "metadata": {
        "id": "_bykMAC8g5C5"
      },
      "id": "_bykMAC8g5C5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_temperature.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EsIATRYgiyFD",
        "outputId": "cf31cb7f-6833-4f91-d21b-aa396db30710"
      },
      "id": "EsIATRYgiyFD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get_current_temperature(latitude: float, longitude: float) -> dict - Fetch current temperature for given coordinates.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "format_tool_to_openai_function(get_current_temperature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKz2P7_1i0Qk",
        "outputId": "0f3d9780-fef2-4b35-ccf7-770ee052ef9e"
      },
      "id": "NKz2P7_1i0Qk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'get_current_temperature',\n",
              " 'description': 'get_current_temperature(latitude: float, longitude: float) -> dict - Fetch current temperature for given coordinates.',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
              "    'type': 'number'},\n",
              "   'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
              "    'type': 'number'}},\n",
              "  'required': ['latitude', 'longitude']}}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_temperature({\"latitude\": 13, \"longitude\": 14})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jIa4Xbrvi0js",
        "outputId": "f6e7143c-178b-4cfc-e57a-91a461b2d8f0"
      },
      "id": "jIa4Xbrvi0js",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 19.0°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 위키피디아 검색 도구\n"
      ],
      "metadata": {
        "id": "99rEyNJ221Wa"
      },
      "id": "99rEyNJ221Wa"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q wikipedia"
      ],
      "metadata": {
        "id": "D2WWpVlpi5td"
      },
      "id": "D2WWpVlpi5td",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
        "    page_titles = wikipedia.search(query)\n",
        "    summaries = []\n",
        "    for page_title in page_titles[: 3]:\n",
        "        try:\n",
        "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
        "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
        "        except:\n",
        "            pass\n",
        "    if not summaries:\n",
        "        return \"No good Wikipedia Search Result was found\"\n",
        "    return \"\\n\\n\".join(summaries)"
      ],
      "metadata": {
        "id": "Kn-T7aO5jcO_"
      },
      "id": "Kn-T7aO5jcO_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_wikipedia.run({\"query\": \"langchain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "rcHtE3mnjisp",
        "outputId": "5ecf3b30-8ea8-436f-dbcd-f6e7e2c645b5"
      },
      "id": "rcHtE3mnjisp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: LangChain\\nSummary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nPage: OpenAI\\nSummary: OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the artificial intelligence spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of computational resources on Microsoft\\'s Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman. It was also announced that Microsoft will have a non-voting board seat.\\n\\nPage: DataStax\\nSummary: DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool Function Calling\n",
        "\n",
        "앞에서 만든 도구들을 함수를 통해서 챗지피티를 사용한다.\n",
        "\n",
        "먼저, 도구를 통해서 Tool_input을 만들어야된다. Tool_input은 Tool의 인자로 들어가며 Tool의 값을 출력하게끔 도와준다.\n",
        "\n",
        "Tool_input은 Route를 통해서 어떤 Tool에 들어가야될지 정해준다."
      ],
      "metadata": {
        "id": "1Rx0Prhw2-Jl"
      },
      "id": "1Rx0Prhw2-Jl"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai  import ChatOpenAI\n",
        "from langchain.schema.messages import HumanMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "\n",
        "\n",
        "functions = [format_tool_to_openai_function(f) for f in [search_wikipedia, get_current_temperature]]\n",
        "\n",
        "model = ChatOpenAI(temperature=0, openai_api_key=openai_api_key).bind(functions=functions)"
      ],
      "metadata": {
        "id": "SjZHVQIYj1TW"
      },
      "id": "SjZHVQIYj1TW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU63oFlfkxSE",
        "outputId": "35ac8c2d-8295-4e5c-e2de-a790e03a75c3"
      },
      "id": "zU63oFlfkxSE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'search_wikipedia',\n",
              "  'description': 'search_wikipedia(query: str) -> str - Run Wikipedia search and get page summaries.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'type': 'string'}},\n",
              "   'required': ['query']}},\n",
              " {'name': 'get_current_temperature',\n",
              "  'description': 'get_current_temperature(latitude: float, longitude: float) -> dict - Fetch current temperature for given coordinates.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
              "     'type': 'number'},\n",
              "    'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
              "     'type': 'number'}},\n",
              "   'required': ['latitude', 'longitude']}}]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "\n",
        "chain = prompt | model\n",
        "chain.invoke({\"input\": \"what is the weather in san francisco right now\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdOHeXKUj7Pu",
        "outputId": "aba75adb-e364-4cd7-a948-925d7d9b3831"
      },
      "id": "rdOHeXKUj7Pu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"what is langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1xk601Ekg2R",
        "outputId": "15d6ef5f-a77b-4896-ef49-a47b64b43f6b"
      },
      "id": "-1xk601Ekg2R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"langchain\"\\n}', 'name': 'search_wikipedia'}})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
        "\n",
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
        "\n",
        "result = chain.invoke({\"input\": \"What is the weather in san francisco right now?\"})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU69dX7AknAf",
        "outputId": "f2dc1117-dbc8-43d3-8f76-cff6b562a4c5"
      },
      "id": "YU69dX7AknAf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentActionMessageLog(tool='get_current_temperature', tool_input={'latitude': 37.7749, 'longitude': -122.4194}, log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzF92iPcmmTP",
        "outputId": "0e603633-08d4-47fd-b2a9-ee20cd7be5d9"
      },
      "id": "lzF92iPcmmTP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.agents.AgentActionMessageLog"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AR3uvgMelEzc",
        "outputId": "c1f88bb8-6b79-4898-fbcf-be60ebce77ad"
      },
      "id": "AR3uvgMelEzc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get_current_temperature'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.tool_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV8yHjKklIW9",
        "outputId": "f6e8f27f-217d-4ca2-bd38-c067fff8e036"
      },
      "id": "xV8yHjKklIW9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latitude': 37.7749, 'longitude': -122.4194}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_temperature(result.tool_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YNj-e3NelLNQ",
        "outputId": "efa0ecc6-fde9-4599-b09f-4ae549ad1275"
      },
      "id": "YNj-e3NelLNQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 8.0°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"hi!\"})\n",
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiO6q78PmwWf",
        "outputId": "ecf6d060-00db-4f87-bbdd-5edbddaaec66"
      },
      "id": "RiO6q78PmwWf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.agents.AgentFinish"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvW58EuxnRBX",
        "outputId": "32b425ef-fc59-40d2-d6e1-fc8ab0254a78"
      },
      "id": "vvW58EuxnRBX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentFinish(return_values={'output': 'Hello! How can I assist you today?'}, log='Hello! How can I assist you today?')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.agent import AgentFinish\n",
        "\n",
        "def route(result):\n",
        "    if isinstance(result, AgentFinish):\n",
        "        return result.return_values['output']\n",
        "\n",
        "    else:\n",
        "        tools = {\n",
        "            \"search_wikipedia\": search_wikipedia,\n",
        "            \"get_current_temperature\": get_current_temperature,\n",
        "        }\n",
        "\n",
        "        return tools[result.tool].run(result.tool_input)"
      ],
      "metadata": {
        "id": "Oh0hiQkIlMx7"
      },
      "id": "Oh0hiQkIlMx7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser() | route"
      ],
      "metadata": {
        "id": "hkEeWtZ0lVIS"
      },
      "id": "hkEeWtZ0lVIS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"What is the weather in san francisco right now?\"})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Idx--x9plVgq",
        "outputId": "51176e8d-3e63-459b-c2fd-aff1c314e9a2"
      },
      "id": "Idx--x9plVgq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 8.0°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 심화"
      ],
      "metadata": {
        "id": "kVw1sDDCP0GA"
      },
      "id": "kVw1sDDCP0GA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool 에이전트 만들기\n",
        "\n",
        "에이전트는 이전 상호 작용에 대해서는 아무것도 기억하지 못합니다.\n",
        "\n",
        "즉, 후속 질문을 쉽게 요청할 수 없습니다. 메모리를 추가하여 문제를 해결합시다."
      ],
      "metadata": {
        "id": "LUhS0ox93a0t"
      },
      "id": "LUhS0ox93a0t"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import MessagesPlaceholder\n",
        "\n",
        "propmt = ChatPromptTemplate.from_messages([\n",
        "\t\t(\"system\", \"You are helpful but sassy assistant\"),\n",
        "\t\t(\"user\", \"{input}\"),\n",
        "\t\tMessagesPlaceholder(variable_name=\"agent_scratchpad\") # 메세지 리스트를 전달하는 공간\n",
        "])\n",
        "\n",
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
        "\n",
        "result1 = chain.invoke({\n",
        "    \"input\": \"what is the weather is sf?\",\n",
        "    \"agent_scratchpad\": []\n",
        "})\n",
        "result1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWXWNQ-jlWzi",
        "outputId": "c5b73391-7c18-4b39-b5c6-fe08f7dea0fb"
      },
      "id": "OWXWNQ-jlWzi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentActionMessageLog(tool='get_current_temperature', tool_input={'latitude': 37.7749, 'longitude': -122.4194}, log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation = get_current_temperature(result1.tool_input)\n",
        "observation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-bHCudHypMXo",
        "outputId": "14654f71-5f5e-4b4b-c68a-f6c54a125499"
      },
      "id": "-bHCudHypMXo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 10.5°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
        "\n",
        "result1.message_log # Agent action으로 나온 결과에 어떻게 도달했는지 알려주는 메시지"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywoT8aGqo7uW",
        "outputId": "d47717c0-410a-4ad7-b9a9-ca0d20bab8fb"
      },
      "id": "ywoT8aGqo7uW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "format_to_openai_functions([(result1, observation), ]) # 메시지와 observation의 튜플을 넘겨주고, 반복하기 때문에 컴마를 붙여줌 무슨소린지는 잘모르겟음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aonebNMmpGas",
        "outputId": "9bb57575-c450-4eaf-aa07-fabe63e90821"
      },
      "id": "aonebNMmpGas",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}}),\n",
              " FunctionMessage(content='The current temperature is 10.5°C', name='get_current_temperature')]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = chain.invoke({\n",
        "    \"input\": \"what is the weather is sf?\",\n",
        "    \"agent_scratchpad\": format_to_openai_functions([(result1, observation)])\n",
        "})\n",
        "\n",
        "result2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghWeb1ZhpIgE",
        "outputId": "9ce41cb0-4bb8-4409-f21b-75ee8d3cb208"
      },
      "id": "ghWeb1ZhpIgE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentActionMessageLog(tool='get_current_temperature', tool_input={'latitude': 37.7749, 'longitude': -122.4194}, log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})])"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
      ],
      "metadata": {
        "id": "30VYpRFottd9"
      },
      "id": "30VYpRFottd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnablePassthrough\n",
        "\n",
        "이 코드는 `RunnablePassthrough`를 사용하여 새로운 키(`agent_scratchpad`)를 추가하고, 이 키에 대한 값을 설정한 다음, 새로운 체인(chain)에 연결하는 것 같습니다.\n",
        "\n",
        "그리고 `|` 연산자를 사용하여 이 새로운 `RunnablePassthrough` 인스턴스를 기존의 체인에 연결하고 있습니다. 따라서, `agent_chain`에는 이전 체인(`chain`)에 새로운 `RunnablePassthrough`가 추가된 새로운 체인이 형성되게 됩니다."
      ],
      "metadata": {
        "id": "EtxSD8VjnNOa"
      },
      "id": "EtxSD8VjnNOa"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "runnable_intermediate =  RunnablePassthrough.assign(agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"]))\n",
        "\n",
        "agent_chain = runnable_intermediate | chain"
      ],
      "metadata": {
        "id": "3mjyq7y6s0Qs"
      },
      "id": "3mjyq7y6s0Qs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.agent import AgentFinish\n",
        "\n",
        "def run_agent(user_input):\n",
        "\n",
        "    intermediate_steps = []\n",
        "\n",
        "    while True:\n",
        "\n",
        "        result = agent_chain.invoke({\n",
        "            \"input\": user_input,\n",
        "            \"intermediate_steps\": intermediate_steps\n",
        "        })\n",
        "\n",
        "        print(result)\n",
        "        print(type(result))\n",
        "\n",
        "        if isinstance(result, AgentFinish):\n",
        "            return result\n",
        "\n",
        "        tool = {\n",
        "            \"search_wikipedia\": search_wikipedia,\n",
        "            \"get_current_temperature\": get_current_temperature,\n",
        "        }[result.tool]\n",
        "\n",
        "        observation = tool.run(result.tool_input)\n",
        "        intermediate_steps.append((result, observation))\n",
        "\n",
        "run_agent(\"what is the weather is sf?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng3zQ1Bvq53S",
        "outputId": "07f5c295-a19a-4246-f1e4-d66145c2d996"
      },
      "id": "Ng3zQ1Bvq53S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tool='get_current_temperature' tool_input={'latitude': 37.7749, 'longitude': -122.4194} log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 37.7749,\\n  \"longitude\": -122.4194\\n}', 'name': 'get_current_temperature'}})]\n",
            "<class 'langchain_core.agents.AgentActionMessageLog'>\n",
            "return_values={'output': 'The current temperature in San Francisco is 10.5°C.'} log='The current temperature in San Francisco is 10.5°C.'\n",
            "<class 'langchain_core.agents.AgentFinish'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 10.5°C.'}, log='The current temperature in San Francisco is 10.5°C.')"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "tools = [get_current_temperature, search_wikipedia]\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)\n",
        "\n",
        "agent_executor.invoke({\"input\": \"what is Langchain\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y95xKomZp5IY",
        "outputId": "29be823d-1a6d-4110-9f3c-7fbc44ee0212"
      },
      "id": "y95xKomZp5IY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `search_wikipedia` with `{'query': 'Langchain'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mPage: LangChain\n",
            "Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
            "\n",
            "\n",
            "\n",
            "Page: OpenAI\n",
            "Summary: OpenAI is an American artificial intelligence (AI) research organization consisting of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. OpenAI researches artificial intelligence with the declared intention of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". OpenAI has also developed several large language models, such as ChatGPT and GPT-4, as well as advanced image generation models like DALL-E 3, and in the past published open-source models.The organization was founded in December 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of compute resources on Microsoft's Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman. OpenAI also announced that Microsoft would have a non-voting board seat at OpenAI.\n",
            "\n",
            "Page: Prompt engineering\n",
            "Summary: Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.A prompt for a text-to-text model can be a query such as \"what is Fermat's little theorem?\", a command such as \"write a poem about leaves falling\", a short statement of feedback (for example, \"too verbose\", \"too formal\", \"rephrase again\", \"omit this word\") or a longer statement including context, instructions, and input data. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as \"maison -> house, chat -> cat, chien ->\", an approach called few-shot learning.When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\n",
            "\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mI couldn't find specific information about \"Langchain\" in my search results. It's possible that Langchain is a less well-known term or a specific term used in a certain context. If you have any additional information or context about Langchain, I may be able to provide more assistance.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is Langchain',\n",
              " 'output': 'I couldn\\'t find specific information about \"Langchain\" in my search results. It\\'s possible that Langchain is a less well-known term or a specific term used in a certain context. If you have any additional information or context about Langchain, I may be able to provide more assistance.'}"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEafGfmUg_RJ"
      },
      "id": "vEafGfmUg_RJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Tool Error\n",
        "\n",
        "도구가 오류를 만나고 예외가 처리되지 않으면, 에이전트는 실행을 중단합니다. 에이전트가 계속 실행되길 원한다면 ToolException을 발생시키고 handle_tool_error를 해당하는 대로 설정할 수 있습니다.\n",
        "\n",
        "ToolException이 발생하면, 에이전트는 작업을 멈추지 않고 도구의 handle_tool_error 변수에 따라 예외를 처리하며 처리 결과는 관측값으로 에이전트에 반환되며 빨간색으로 출력됩니다.\n",
        "\n",
        "handle_tool_error를 True로 설정하거나 통일된 문자열 값으로 설정하거나 함수로 설정할 수 있습니다. 함수로 설정할 경우, 함수는 ToolException을 매개변수로 받고 str 값을 반환해야 합니다."
      ],
      "metadata": {
        "id": "PNvlqYv0vumi"
      },
      "id": "PNvlqYv0vumi"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import Tool\n",
        "from langchain.tools.base import ToolException\n",
        "\n",
        "\n",
        "# 직접 에러문을 생성해서 사용할 수 있음\n",
        "def _handle_error(error: ToolException) -> str:\n",
        "    return (\n",
        "        \"The following errors occurred during tool execution:\"\n",
        "        + error.args[0]\n",
        "        + \"Please try another tool.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def search_tool1(s: str):\n",
        "    raise ToolException(\"The search tool1 is not available.\")\n",
        "\n",
        "\n",
        "def search_tool2(s: str):\n",
        "    raise ToolException(\"The search tool2 is not available.\")\n",
        "\n",
        "\n",
        "search_tool3 = search_wikipedia"
      ],
      "metadata": {
        "id": "kSEYTxsWqiAI"
      },
      "id": "kSEYTxsWqiAI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"useful for when you need to answer questions about current events.You should give priority to using it.\"\n",
        "tools = [\n",
        "    Tool.from_function(\n",
        "        func=search_tool1,\n",
        "        name=\"Search_tool1\",\n",
        "        description=description,\n",
        "        handle_tool_error=True,\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=search_tool2,\n",
        "        name=\"Search_tool2\",\n",
        "        description=description,\n",
        "        handle_tool_error=_handle_error,\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=search_tool3.run,\n",
        "        name=\"Search_tool3\",\n",
        "        description=\"useful for when you need to answer questions about current events\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    ChatOpenAI(temperature=0, openai_api_key=openai_api_key),\n",
        "    # agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    agent=AgentType.OPENAI_FUNCTIONS,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Eap9i8M9waGb"
      },
      "id": "Eap9i8M9waGb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Who is Leo DiCaprio's girlfriend?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "GjAgP2uywteM",
        "outputId": "8ddce3d0-4141-4d46-e24f-fe1d24bbdfa4"
      },
      "id": "GjAgP2uywteM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Search_tool1` with `Leo DiCaprio girlfriend`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[31;1m\u001b[1;3mThe search tool1 is not available.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Search_tool2` with `Leo DiCaprio girlfriend`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[31;1m\u001b[1;3mThe following errors occurred during tool execution:The search tool2 is not available.Please try another tool.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Search_tool3` with `Leo DiCaprio girlfriend`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mPage: Leonardo DiCaprio\n",
            "Summary: Leonardo Wilhelm DiCaprio (; Italian: [diˈkaːprjo]; born November 11, 1974) is an American actor and film producer. Known for his work in biographical and period films, he is the recipient of numerous accolades, including an Academy Award, a British Academy Film Award, and three Golden Globe Awards. As of 2019, his films have grossed over $7.2 billion worldwide, and he has been placed eight times in annual rankings of the world's highest-paid actors.\n",
            "Born in Los Angeles, DiCaprio began his career in the late 1980s by appearing in television commercials. In the early 1990s, he had recurring roles in various television shows, such as the sitcom Parenthood, and had his first major film part as author Tobias Wolff in This Boy's Life (1993). He received critical acclaim and his first Academy Award and Golden Globe Award nominations for his performance as a developmentally disabled boy in What's Eating Gilbert Grape (1993). DiCaprio achieved international stardom with the star-crossed romances Romeo + Juliet (1996) and Titanic (1997). After the latter became the highest-grossing film in the world at the time, he reduced his workload for a few years. In an attempt to shed his image of a romantic hero, DiCaprio sought roles in other genres, including the 2002 crime dramas Catch Me If You Can and Gangs of New York; the latter marked the first of his many successful collaborations with director Martin Scorsese.\n",
            "DiCaprio continued to gain acclaim for his performances in the biopic The Aviator (2004), the political thriller Blood Diamond (2006), the crime drama The Departed (2006) and the romantic drama Revolutionary Road (2008). He later made environmental documentaries and starred in several high-profile directors' successful projects, including the action thriller Inception (2010), the western Django Unchained (2012), the biopic The Wolf of Wall Street (2013), the survival drama The Revenant (2015)—for which he won the Academy Award for Best Actor— the comedy-dramas Once Upon a Time in Hollywood (2019) and Don't Look Up (2021), and the crime drama Killers of the Flower Moon (2023).\n",
            "DiCaprio is the founder of Appian Way Productions—a production company that has made some of his films and the documentary series Greensburg (2008–2010)—and the Leonardo DiCaprio Foundation, a nonprofit organization devoted to promoting environmental awareness. A United Nations Messenger of Peace, he regularly supports charitable causes. In 2005, he was named a Commander of the Ordre des Arts et des Lettres for his contributions to the arts, and in 2016, he appeared in Time magazine's 100 most influential people in the world. DiCaprio was voted one of the 50 greatest actors of all time in a 2022 readers' poll by Empire.\n",
            "\n",
            "Page: Camila Morrone\n",
            "Summary: Camila Rebeca Morrone Polak (; born June 16, 1997) is an American actress and model. She is the daughter of actors Maximo Morrone and Lucila Polak; and was also raised in part by actor Al Pacino (who served as a parental figure during his long-term partnership with Polak). Morrone made her acting debut in the James Franco film Bukowski (2013). In 2016, she posed in the Victoria's Secret lookbook; and walked the runway for Moschino, the following year. Since then, she has made appearances in the action film Death Wish (2018), as well as the independent films Never Goin' Back (2018) and Mickey and the Bear (2019). Her role as Camila Alvarez-Dunne in the Amazon Prime Video limited series Daisy Jones & the Six (2023), earned her a nomination for a Primetime Emmy Award.\n",
            "\n",
            "Page: The Departed\n",
            "Summary: The Departed is a 2006 American epic crime thriller film directed by Martin Scorsese and written by William Monahan. It is both a remake of the 2002 Hong Kong film Infernal Affairs and also loosely based on the real-life Boston Winter Hill Gang; the character Colin Sullivan is based on the corrupt FBI agent John Connolly, while the character Frank Costello is based on Irish-American gangster and crime boss Whitey Bulger. The film stars Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg, with Martin Sheen, Ray Winstone, Vera Farmiga, Alec Baldwin, Anthony Anderson and James Badge Dale in supporting roles.\n",
            "The film takes place in Boston and the surrounding metro area, primarily in the South Boston neighborhood. Irish Mob boss Frank Costello (Nicholson) plants Colin Sullivan (Damon) as a spy within the Massachusetts State Police; simultaneously, the police assign undercover state trooper Billy Costigan (DiCaprio) to infiltrate Costello's mob crew. When both sides realize the situation, Sullivan and Costigan each attempt to discover the other's identity before they are found out.\n",
            "The Departed was a critical and commercial success, receiving acclaim for its direction, performances (particularly of DiCaprio, Nicholson, and Wahlberg), screenplay, and editing.\n",
            "It won several accolades, including four Oscars at the 79th Academy Awards: for Best Picture, Best Director, Best Adapted Screenplay, and Best Film Editing. It became Scorsese's first and, to date, only personal Oscar win; Wahlberg was also nominated for Best Supporting Actor. The film also received six nominations at the 64th Golden Globe Awards, six nominations at the 60th British Academy Film Awards, and two nominations at the 13th Screen Actors Guild Awards. DiCaprio was nominated for Golden Globe Award for Best Actor – Motion Picture Drama (also nominated that year in the same category for Blood Diamond), BAFTA Award for Best Actor in a Leading Role and Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Supporting Role for his performance.\n",
            "\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mAccording to the information I found, Leonardo DiCaprio's girlfriend is Camila Morrone. She is an American actress and model.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"According to the information I found, Leonardo DiCaprio's girlfriend is Camila Morrone. She is an American actress and model.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom LLM Agent 만들어 보기\n",
        "\n",
        "LLM 채팅 에이전트는 다음과 같은 네 가지 주요 구성 요소로 구성됩니다:\n",
        "\n",
        "1. PromptTemplate: 언어 모델에 수행 할 작업을 지시하는 프롬프트 템플릿입니다.\n",
        "\n",
        "2. ChatModel: 에이전트에 전원을 공급하는 언어 모델입니다.\n",
        "\n",
        "3. stop시퀀스 :이 문자열을 찾 자마자 LLM에 생성을 중지하도록 지시합니다.\n",
        "\n",
        "4. OutputParser: LLM 출력을 AgentAction 또는 AgentFinish 목적."
      ],
      "metadata": {
        "id": "orz6h0I4_vi9"
      },
      "id": "orz6h0I4_vi9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM 에이전트AgentExecutor 다음과 같은 루프로 생각할 수 있습니다\n",
        "\n",
        "1. 사용자 입력 및 이전 단계를 에이전트 (이 경우 LLM 에이전트)에 전달합니다.\n",
        "\n",
        "2. 에이전트가 AgentFinish, 그런 다음 사용자에게 직접 반환\n",
        "\n",
        "3. 에이전트가 AgentAction, 그런 다음이를 사용하여 도구를 호출하고 Observation\n",
        "\n",
        "4. 반복 AgentAction 과 Observation 요원까지 AgentFinish 리턴됩니다.\n",
        "\n",
        "AgentFinish 사용자에게 다시 보낼 최종 메시지가 포함 된 응답입니다. 에이전트 실행을 종료하는 데 사용해야합니다.\n",
        "\n",
        "https://python.langchain.com/docs/modules/agents/how_to/custom_llm_chat_agent"
      ],
      "metadata": {
        "id": "R7IfRYiCCopK"
      },
      "id": "R7IfRYiCCopK"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
        "from langchain.prompts import BaseChatPromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from typing import List, Union\n",
        "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
        "import re\n",
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
      ],
      "metadata": {
        "id": "uc5GcA6tCoC1"
      },
      "id": "uc5GcA6tCoC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
        "    page_titles = wikipedia.search(query)\n",
        "    summaries = []\n",
        "    for page_title in page_titles[: 1]:\n",
        "        try:\n",
        "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
        "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
        "        except:\n",
        "            pass\n",
        "    if not summaries:\n",
        "        return \"No good Wikipedia Search Result was found\"\n",
        "    return \"\\n\\n\".join(summaries)"
      ],
      "metadata": {
        "id": "xfFXet2XGT1P"
      },
      "id": "xfFXet2XGT1P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM에게 템플리슬 통해 유도하는 방식으로 Function Calling을 쓰지않고 사용했지만, 예시기 때문에 안쓴거고 사실 쓰는 것이 좋음\n",
        "\n",
        "template = \"\"\"Complete the objective as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "These were previous tasks you completed:\n",
        "\n",
        "\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\""
      ],
      "metadata": {
        "id": "p_aMsjSzXDLH"
      },
      "id": "p_aMsjSzXDLH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format_messages(self, **kwargs) -> str:\n",
        "        # 중간 단계(에이전트 작업, 관찰 튜플) 가져오기\n",
        "        # 특정 방식으로 형식 지정\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "\n",
        "        # agent_scratchpad  변수를 해당 값으로 설정합니다.\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "\n",
        "        # 제공된 도구 목록에서 도구 변수를 만들기\n",
        "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "\n",
        "        # 제공된 도구의 도구 이름 목록 만들기\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        formatted = self.template.format(**kwargs)\n",
        "        return [HumanMessage(content=formatted)]"
      ],
      "metadata": {
        "id": "b2KpgnIHDPGp"
      },
      "id": "b2KpgnIHDPGp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [get_current_temperature, search_wikipedia]"
      ],
      "metadata": {
        "id": "TS_4-B3cDuzK"
      },
      "id": "TS_4-B3cDuzK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = CustomPromptTemplate(\n",
        "    template=template,\n",
        "    tools=tools,\n",
        "    # 여기에는 `agent_scratchpad`, `tools` 및 `tool_names` 변수는 동적으로 생성되므로 생략합니다.\n",
        "    # 여기에는 `intermediate_steps` 변수가 필요하므로 포함됩니다.\n",
        "    input_variables=[\"input\", \"intermediate_steps\"]\n",
        ")"
      ],
      "metadata": {
        "id": "p1zdS3OuDRXm"
      },
      "id": "p1zdS3OuDRXm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomOutputParser(AgentOutputParser):\n",
        "\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "\n",
        "        # Check if agent should finish\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # 반환 값은 일반적으로 항상 단일 `output` 키가 있는 딕셔너리입니다.\n",
        "                # 현재로서는 다른 것을 시도하는 것은 권장하지 않습니다 :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "\n",
        "        # 검색할 것 요소를 찾는다.\n",
        "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "\n",
        "        # Return the action and action input\n",
        "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
      ],
      "metadata": {
        "id": "ZB55xuSVDbWP"
      },
      "id": "ZB55xuSVDbWP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = CustomOutputParser()"
      ],
      "metadata": {
        "id": "9DL3kanpDj14"
      },
      "id": "9DL3kanpDj14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)"
      ],
      "metadata": {
        "id": "RmyerbzGDmQw"
      },
      "id": "RmyerbzGDmQw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "QG7wYnihDok_"
      },
      "id": "QG7wYnihDok_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_names = [tool.name for tool in tools]\n",
        "\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names\n",
        ")"
      ],
      "metadata": {
        "id": "AHvmcrkdDqJq"
      },
      "id": "AHvmcrkdDqJq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "mZw6b3eaDw9w"
      },
      "id": "mZw6b3eaDw9w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run(\"what is langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "u8v86gm3Dz38",
        "outputId": "31380dfd-5043-423b-f2ee-d5dfcbf8ff2f"
      },
      "id": "u8v86gm3Dz38",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I don't know what \"langchain\" is, so I should search for it on Wikipedia.\n",
            "Action: search_wikipedia\n",
            "Action Input: \"langchain\"\u001b[0m\n",
            "\n",
            "Observation:\u001b[33;1m\u001b[1;3mPage: LangChain\n",
            "Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe Wikipedia search for \"langchain\" returned a page called \"LangChain\" which provides a summary of what it is.\n",
            "Final Answer: LangChain is a framework designed to simplify the creation of applications using large language models.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is a framework designed to simplify the creation of applications using large language models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dccZY3hWE2Rw"
      },
      "id": "dccZY3hWE2Rw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kVw1sDDCP0GA"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}